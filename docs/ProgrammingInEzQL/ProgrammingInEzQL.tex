\documentclass{report}

\usepackage{framed}
\usepackage{alltt}

\newcommand{\tuple}[1]{$(#1)$}

\newenvironment{evaluation}
{
  \framed
  \begin{alltt}
}
{
  \end{alltt}
  \endframed
}

\begin{document}
\title{Introduction to EzQL}
\author{Lu\'{\i}s Pureza}

\maketitle

\chapter*{Disclaimer}

EzQL is still being designed and will remain so during the rest of
this internship. There are some operators that have not yet been
completely defined either because their semantics are not clear or
because they are simply low priority right now. Also, we are still
trying to discover new problems and thinking about the best way to
solve them using our language. As such, this document is not yet in
its final version. Currently, chapters 1-4 are mostly written.

\tableofcontents

\addtolength{\parskip}{\baselineskip}

\chapter{Introduction}
\label{chap:introduction}
EzQL is a new programming language targeted at event stream processing
(ESP) applications. As explained in some detail in the main report
that accompanies this tutorial, currently available ESP languages lack
expressiveness, making the task of writing simple and reasonable
queries rather cumbersome. We believe one of the main reasons this
happens is because the semantics of constructs provided are weaker
than the semantics that would be necessary to answer those
queries. That is, there is an impedance mismatch between what the
existing languages provide and what the developers really need. EzQL
is our proposal to narrow the gap.

The main features of EzQL are:

\begin{itemize}
\item Combines the \emph{imperative} (C, Java) and the
  \emph{declarative} (SQL) paradigms allowing the developer to query
  data with a SQL-like language or by writing an algorithm, specified
  as a sequence of small steps, for those cases where SQL is not
  appropriate;
\item Includes a simple \emph{object-model} where real-world entities
  can be modeled as objects with properties --- temperature, price,
  speed, \ldots ---, whose values depend on the values of events. For
  instance, this object-model allows the creation of many Room objects
  each with their own temperature attribute which is automatically
  updated as new sensor readings arrive. Furthermore, these objects
  and the values of their properties define the state of the
  application and this state can be queried just like regular events;
\item The object model supports \emph{associations} between objects,
  inspired by Object-Relational Mapping (O/RM) tools for
  databases. Thus, it is possible to say, for instance, that a Room
  has many Products and, correspondingly, that a Product belongs to
  one Room. Again, these associations may change as new events arrive,
  i.e., a Product may leave one Room and enter another. Naturally,
  these relationships can also be queried --- one can ask, for
  example, which is the cheapest Product inside Room A. This type of
  queries can already be written in currently available systems, but
  we will see that they become much simpler in EzQL;
\item EzQL provides extensive support for \emph{temporal queries}, a
  common class of queries in ESP applications that current products
  don't handle so well, as discussed in the main report. This support
  integrates with the object-model so that one can easily ask, for
  example, for how long was the temperature in some room above
  20$^{\circ}$C during the last 10 minutes;
\end{itemize}

The purpose of this document is to describe EzQL in detail, so that
developers can understand its syntax and semantics and start using it
right away to solve new problems.

The rest of this chapter discusses the rationale behind the design of
EzQL and some of its current limitations. Then, the document proceeds
with chapter \ref{chap:streams-events} that explains what exactly are
streams and events and what can you do with them, including how to
query them. Chapter \ref{chap:objects} describes how to model
real-world entities as objects in EzQL and how to query their
state. Chapter \ref{chap:continuous-values} discusses \emph{Continuous
  values}, a new kind of data abstraction that allows one to query the
past state of the application. Finally, the topic of chapter
\ref{chap:udfs} is how to extend the core set of operations provided
by the EzQL environment by writing your own functions and aggregators.

\section{Design goals}
\label{sec:design-goals}

Any new programming language such as EzQL must be designed with a set
of goals in mind. Some of these are quite general --- ``Be useful!'',
while others are chosen carefully by the designers to set the new
language apart from any other, like Python's ``There should be one ---
and preferably only one --- obvious way to do it''. These principles
become a major influence in the resulting language and even define
what developers sometimes call the philosophy of the language. It is
important that the language adheres to its principles. Otherwise, it
just won't make sense.

These are the goals that drove the design of the EzQL language:

\begin{description}
\item[Be powerful] The set of operations allowed in a query should be
  large enough to accommodate all reasonable things users might want
  to do with their data. Basically, frequent operations should be
  simple and rare operations should be possible;
\item[Be extensible] ESP is a novel area with unique requirements that
  are still being discovered. No one really knows the boundary between
  what is ESP and what is not, but every day ESP systems are applied
  to new problems with their own characteristics. It is mandatory that
  the language is extensible enough to accommodate this evolution,
  much as general purpose languages adapt to new domains through the
  development of new libraries;
\item[Be expressive] EzQL queries should be straightforward to read,
  write and understand by the developer. In particular, it should be
  possible to partition big queries into smaller ones in a
  step-by-step fashion --- like a procedural programming language,
  where each statement specifies a new operation acting on the result
  of the previous statement. Also, common operations should be
  expressible directly by what they mean and not as an opaque set of
  SQL-like constructs that hide their real meaning;
\item[Be coherent] In Smalltalk, everything is an object and even the
  addition of two numbers is seen as messages flowing between
  them. Lisp programs are just data and any Lisp fan will happily
  explain why that has so many advantages. Java, on the other hand,
  distinguishes between built-in types like integers and user-defined
  types and this distinction frequently bothers developers. Coherency
  makes the language easier to understand because you have less rules
  and exceptions. But it's far more than that: a coherent language
  shows how well-thought it was. This kind of elegance cannot be
  ignored.
\end{description}

% One major source of inspiration during the design of EzQL was
% Microsoft's LINQ [ref]. LINQ --- Language INtegrated Query ---, is a
% library for the .NET platform that provides a SQL-like language to
% query not only databases, but also in-memory collections like lists or
% dictionaries, XML files and even online services like Amazon
% store. LINQ for databases includes an O/RM module that creates objects
% from database entities and manages their properties and
% relationships. These objects may, of course, be used in normal LINQ
% queries just like raw entities, something we want EzQL be able to do
% as well. Furthermore, note that LINQ comes with its own SQL-like query
% language, but this language is expected to be used in conjunction with
% other .NET languages, like C# or VB.NET. This way, the developer may
% choose to use LINQ for all his data processing tasks or he may opt to
% use LINQ for some tasks and rely on good old imperative constructs to
% implement more complex operations, which is also one of the main
% characteristics of EzQL. In fact, the similarities between LINQ and
% EzQL are so many that it could be said that EzQL is in essence a LINQ
% for ESP.

%http://msdn.microsoft.com/en-us/library/bb425822.aspx#linqtosql_topic5
% [Table(Name="Orders")]
% public class Order
% {
%    [Column(Id=true)]
%    public int OrderID;
%    [Column]
%    public string CustomerID;
%    private EntityRef<Customer> _Customer;    
%    [Association(Storage="_Customer", ThisKey="CustomerID")]
%    public Customer Customer {
%       get { return this._Customer.Entity; }
%       set { this._Customer.Entity = value; }
%    }
% }

% var q =
%    from c in db.Customers
%    from o in c.Orders
%    where c.City == "London"
%    select new { c, o };



By the end of this document, EzQL should just make sense, i.e., all
its features should fit together and complement each other to create a
language that achieves the goals listed above. Every feature or
construct should have a well-defined purpose and be generally
applicable to a wide-range of ESP problems.

\section{Current limitations}
\label{sec:current-limitations}

These are some of the current limitations of EzQL:

\begin{itemize}
\item No pattern matching to detect complex patterns of events. In the
  future, one will probably integrate a pattern language such as SASE+
  [ref] directly into EzQL, much like regular expression libraries
  provide one mini-language inside another;
\item It doesn't integrate with databases. This is a serious drawback
  because not all information is available in events. For instance,
  when a costumer places an order, an event may be generated for the
  system to process. But what if the application needs to know the
  name of the costumer? Should it be part of the event? What about the
  address? What about the previous orders the costumer placed?
  Clearly, this information should already be available in the
  database and thus it is undesirable to enlarge the events with
  redundant information. Nonetheless, currently EzQL does not address
  this issue;
\item Out-of-order events: still an ongoing research topic. EzQL
  assumes all events are received in order.
\end{itemize}

\chapter{Streams and events}
\label{chap:streams-events}

\section{Preliminaries}

An event (also called a tuple) signals that something
happened. Exactly what happened is encoded in the event itself as a
list of field names packed together with their corresponding
values. Moreover, all events have an implicit times\-tamp for when the
event occurred. For example, an event signaling that Apple stocks were
priced at \$50 at 10:59:59 am today, may be represented with the
following encoding:

\begin{verbatim}
{ :symbol => "AAPL", :price => 50 } at 01/11/2009 10:59:59 am
\end{verbatim}

This event includes two fields: \verb=:symbol= with value
\verb="AAPL"= and \verb=:price=, with value \verb=50=. Field names are
always prefixed with a colon. From now on we will assume all events
happened today and will omit the date from their timestamp when we
print them.

To access an event's field one uses the . (dot) operator:

\begin{evaluation}
ezql> event = \{ :symbol => "AAPL", :price => 50 \}
ezql> event.symbol
"AAPL"
ezql> event.price
50
ezql> event.timestamp
<the date and time the event was created in the first line>
\end{evaluation}

As can be seen in the example, the timestamp is available as if it
were a normal field.

A stream is an abstract data type that collects events. Informally, a
stream could be seen as a list of events ordered by their timestamp,
i.e., older events at one end, newer events at the other end. However,
this is not entirely accurate because streams are active entities and
possibly infinite. For instance, a stream that collects events from
financial markets may receive thousands of new events every second
and, what's more important, this flow of events may never terminate.

The following example shows how to declare streams:

\begin{verbatim}
stocks = new Stream of { :symbol, :price }
\end{verbatim}

This declares a stream called \verb=stocks= that receives events
containing two fields (besides the implicit timestamp): \verb=:symbol=
and \verb=:price=. Now you may develop queries over this stream using
the operators discussed next.

\section{Query operations}
\label{sec:query-operations}
The query operations include all the SQL-like constructs that can be
used to manipulate data.

The result of each operation will be illustrated with examples of
applications over a stream with stock data where each tuple contains
two fields, \verb=:symbol= and \verb=:price=. It is assumed that this
stream, \verb=stocks=, will receive the following tuples:

\begin{verbatim}
[{ :symbol => "AAPL", :price => 50 } at 10:59:59 am,
 { :symbol => "AAPL", :price => 52 } at 11:00:02 am,
 { :symbol => "MSFT", :price => 70 } at 11:00:03 am,
 { :symbol => "MSFT", :price => 75 } at 11:00:06 am,
 { :symbol => "AAPL", :price => 50 } at 11:00:07 am]
\end{verbatim}

Naturally, these values are only for illustration purposes and are not
representative of the real market.

\subsection{Mapping}
\label{sec:mapping}
A mapping applies a function to all tuples in a stream and returns a
new stream with the results. For instance, one could double the
\verb=:price= of stock data with:

\begin{verbatim}
price2x = stocks
            .select (\t -> { :symbol => t.symbol,
                             :price => t.price * 2 })
\end{verbatim}

The first thing to clarify about this example is that the piece of
code inside parenthesis, \verb!\t -> { ... }! is actually an anonymous
or lambda function that receives one argument --- \verb=t= ---, and
produces a new tuple with fields \verb=:symbol=, copied from \verb=t=,
and \verb=:price= which is equal to twice the price of the
argument. \verb=select= receives this anonymous function, calls it for
every tuple received in the \verb=stocks= stream and saves the results
to a new stream, which it then returns. \verb=select= is really just a
continuous version of LISP's \verb=mapcar= or Python's \verb=map=.

After the sample events arrive on the \verb=stocks= stream,
\verb=price2x= will contain the following tuples\footnote{Unlike other
  ESP products, streams in EzQL retain all their events by default. It
  is up to the runtime to decide when some event is not needed anymore
  and ``garbage collect'' it.}:

\begin{evaluation}
ezql> price2x
[\{ :symbol => "AAPL", :price => 100 \} at 10:59:59 am,
 \{ :symbol => "AAPL", :price => 104 \} at 11:00:02 am,
 \{ :symbol => "MSFT", :price => 140 \} at 11:00:03 am,
 \{ :symbol => "MSFT", :price => 150 \} at 11:00:06 am,
 \{ :symbol => "AAPL", :price => 100 \} at 11:00:07 am]
\end{evaluation}

Note that timestamps were copied from the original events. In a real
implementation, timestamps could be generated from the system clock
when the events are created by the anonymous function in
\verb=select=. Thus, in general, they may differ from the original.

One can perform further operations on this stream like, for example,
convert the price from euros to dollars using a function called
\verb=eur2dol= defined elsewhere:

\pagebreak
\begin{verbatim}
price2x_dol = price2x
                .select (\t -> { :symbol => t.symbol,
                                 :price => eur2dol(t.price) })
\end{verbatim}

Note that in the previous examples you were just copying
\verb=:symbol= around. This is OK for one field, but could get
tiresome if the number of fields increases. An alternative syntax
might come in handy for these situations:

\begin{verbatim}
price2x_dol = price2x
                .select (\t -> { t with
                                   :price => eur2dol(t.price) })
\end{verbatim}

This should read as ``Copy all the fields from \verb=t= except
\verb=:price=, which should be calculated as follows''.

\subsection{Filters}
\label{sec:filters}

One can also filter tuples in a stream based on a predicate. The
resulting stream will include only those tuples that pass the
predicate. For example, to obtain only the Apple tuples one could
write:

\begin{verbatim}
apple_stocks = stocks
                 .where (\t -> t.symbol == "AAPL")
\end{verbatim}

Note that the predicate is a function that receives an event and
returns a boolean. Like \verb=select=, \verb=where= calls the
predicate for every tuple but adds to the resulting stream only those
tuples where the predicate returns true.

Evaluating the stream \verb=apple_stocks= after the sample events are
received results in:

\begin{evaluation}
ezql> apple_stocks
[\{ :symbol => "AAPL", :price => 50 \} at 10:59:59 am,
 \{ :symbol => "AAPL", :price => 52 \} at 11:00:02 am,
 \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

At this point, it is convenient to pause and reflect a little about
the execution model of EzQL programs. Imagine the following query,
that doubles the price of Apple events:

\begin{verbatim}
apple_stocks_2x =
  stocks
    .where  (\t -> t.symbol == "AAPL")
    .select (\t -> { t with :price => t.price * 2 }
\end{verbatim}

Unlike regular programming languages, EzQL statements like the
previous one are not executed sequentially. This query should not read
as ``Take all events in the \verb=stocks= stream, then filter out
those that do not belong to Apple and finally double the price of the
remaining stocks''. This is because, as described in the beginning of
the chapter, streams receive new events constantly. So you can never
say ``Take all events in \verb=stocks= stream'', because this stream
will never be complete. Instead, we must operate on events as they
arrive. Thus, what the previous query really means is: ``As new events
arrive in the \verb=stocks= stream, ignore those not related to Apple,
double the price of the others and put them in the
\verb=apple_stocks_2x= stream''. The difference is subtle but
important. Essentially, it means the previous query is always
running. To see this, imagine you evaluate \verb=apple_stocks= at
10:59:00 am:

\begin{evaluation}
ezql> apple_stocks_2x
[]
\end{evaluation}

As expected, the stream is empty. But if you evaluate it again 5
minutes later, at 11:04:00 am, the stream won't be empty anymore:

\begin{evaluation}
ezql> apple_stocks_2x
[\{ :symbol => "AAPL", :price => 100 \} at 10:59:59 am,
 \{ :symbol => "AAPL", :price => 104 \} at 11:00:02 am,
 \{ :symbol => "AAPL", :price => 100 \} at 11:00:07 am]
\end{evaluation}

\subsection{Aggregators}
\label{sec:aggregators}

Aggregators are functions that are applied to streams and return a
single value. Example aggregators include the usual suspects:
\verb=min()=, \verb=max()=, \verb=sum()=, \verb=avg()=, \ldots.

For example, the average price attained by Apple stocks can be
found out with:

\begin{verbatim}
avg_apple = apple_stocks.avg(:price)
\end{verbatim}

In this example, \verb=avg_apple= is not a standard numeric
value. Instead, its value will change over time, as new tuples
arrive. Thus, if at any moment you need to print the average of
Apple's stock prices, you need only to print \verb=avg_apple= and not
the entire expression on the right side.

The fact that the value of the aggregator changes, suggests that it
may be seen as stream with all its previous values. But new averages
are not really new events and the analogy breaks easily. For example,
if the average of Apple stocks was a stream, one could filter this
stream to obtain the averages greater than a given threshold:

\begin{verbatim}
avg_apple
  .where (\avg -> avg.value > THRESHOLD)
\end{verbatim}

But what if the developer needs to find out for how long was the
average greater than the given threshold? Unfortunately, this cannot
be easily answered if we treat new averages as events, because events
``happen'' at a well-defined time, but provide no information
regarding their duration. Without the lower averages, we lose
continuity and can't know for sure for how long was a given average
valid, because the one that followed it may not be in the stream
anymore. Thus, treating aggregators as streams is a weak analogy.

What we would really like to have is a two dimensional plot of the
evolution of the aggregator's value over time. This way, answering the
question would be a simple matter of cutting the plot in two by the
threshold and sum the lengths of the time intervals where the value
was above the cut. This plot is a \emph{continuous value}, a new kind
of data abstraction for values that are truly continuous, i.e., their
value is defined even in the absence of events. We will resume this
discussion after we've discussed continuous values in chapter
\ref{chap:continuous-values}.

\subsection{Sorting}
\label{sec:sorting}

TODO

Sorting may only be applied to Windows, not streams.

\subsection{Group by}
\label{sec:group-by}

In its simplest form, \emph{group by} acts as demultiplexer
partitioning a single stream into many smaller streams. Events are
sent to one of these streams where an aggregator computes some value
per stream.

For example, to find the maximum stock price for each company one
could write:

\begin{verbatim}
max_by_company =
  stocks
    .groupby(:symbol,
            (\group -> group.max(:price)))
\end{verbatim}

The second parameter received by \emph{group by} is, once again, an
anonymous function that receives a group and specifies what to do with
that group.

The result of this particular \emph{group by} is a dictionary that
maps symbols to maximums:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & Maximum price \\
  \hline
  ``AAPL'' & 52 \\
  ``MSFT'' & 75 \\
  \hline
\end{tabular}

The maximum price achieved by Apple could be discovered writing
simply:

\begin{evaluation}
ezql> max_by_company["AAPL"]
52
\end{evaluation}

Once again, please note that this maximum is actually a continuous
value that may change as new events arrive.

The \emph{group by} operator also allows partitioning by more than one
field at a time:

\begin{verbatim}
price_count_per_company =
  stocks
    .groupby({ :symbol, :price }
            (\group -> group.count()))
\end{verbatim}

Which results in:

\begin{tabular}{ |l|r|c| }
  \hline
  :symbol & :price & count() \\
  \hline
  ``AAPL'' & 50 & 2 \\
  ``AAPL'' & 52 & 1 \\
  ``MSFT'' & 70 & 1 \\
  ``MSFT'' & 75 & 1 \\
  \hline
\end{tabular}

The following query accesses this dictionary to find out how many
times Apple stocks were priced at exactly \$50:

\begin{evaluation}
  ezql> price_count_per_company[\{ :symbol => "AAPL",
                                  :price => 50 \}]
  2
\end{evaluation}

Finally, one can also specify arbitrary partitions:

\begin{verbatim}
hourly_price_per_company =
  stocks
    .groupby(\t -> { :symbol, :hour => t.timestamp.hour() },
            (\group -> group.max(:price)))
\end{verbatim}

The previous example calculates the maximum price per hour. The first
parameter is a function that, when given an event, chooses the group
in which it is going to put the event. This results in:

\begin{tabular}{ |l|r|c| }
  \hline
  :symbol & :hour & max() \\
  \hline
  ``AAPL'' & 10 & 50 \\
  ``AAPL'' & 11 & 52 \\
  ``MSFT'' & 11 & 75 \\
  \hline
\end{tabular}

Once again, accessing the results is straightforward:

\begin{evaluation}
ezql> hourly_price_per_company[\{ :symbol => "MSFT",
                                 :hour   => 11 \}]
75
\end{evaluation}

Despite not being exactly a stream, some stream operations may be
applied onto the result of a \emph{group by}. In particular,
\verb=where= returns a new dictionary with only the groups that pass a
given predicate. For example, the following query returns a dictionary
containing only the companies whose maximum price is above \$70:

\begin{verbatim}
max_by_company
  .where (\k, v -> v > 70)
\end{verbatim}

In this example the predicate is a function that receives two
parameters, \verb=k= and \verb=v=, which stand for the groups' key and
value in the \verb=max_by_companies= dictionary. Note that
\verb=where= when applied to the result of a \emph{group by} acts just
like the \verb=HAVING= operator in SQL.

This following dictionary is the result of the operation:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & Maximum price \\
  \hline
  ``MSFT'' & 75 \\
  \hline
\end{tabular}



TODO: Currently there must be an aggregator at the end of the line. Is
this reasonable? What if we want to return, I don't know, the latest 3
events per group? It should be perfectly acceptable to do so.

\subsection{Joining multiple streams}
\label{sec:join}

TODO - not really that important with the objects model.

Types of joins:
\begin{itemize}
\item equi-join
\item Cross join
\item Left and right outer joins
\item Full outer join
\end{itemize}

\subsubsection{Joining with timestamps}
\label{sec:timestamp-join}

There is a situation where you may join two streams without using
windows: when the timestamp is part of the join predicate. Let's see a
simple example to understand how these kinds of joins work.

Assume that we have two streams --- {\tt temp\_readings} and {\tt
  hum\_readings} ---, that contain temperature and relative humidity
readings for rooms in a building. \verb=temp_readings= receives:

\begin{verbatim}
[{ :room_id => "A", :temp => 20 } at 11:00:00 am,
 { :room_id => "B", :temp => 21 } at 11:01:00 am,
 { :room_id => "A", :temp => 23 } at 11:02:00 am,
 { :room_id => "B", :temp => 22 } at 11:03:00 am]
\end{verbatim}

\verb=hum_readings=, on the other hand, gets the following events:

\begin{verbatim}
[{ :room_id => "B", :hum => 0.60 } at 11:00:30 am,
 { :room_id => "A", :hum => 0.62 } at 11:01:30 am,
 { :room_id => "A", :hum => 0.61 } at 11:02:30 am,
 { :room_id => "B", :hum => 0.59 } at 11:03:00 am]
\end{verbatim}

Now imagine you need to correlate temperatures and humidities. For
instance, you may need to answer the following question: ``what was
the maximum temperature while the humidity was above 60\%?''. To do
this, it might come in handy to ``merge'' both streams to obtain a new
stream with three fields --- \verb=:room_id=, \verb=:temp= and
\verb=:hum=. The events in this new stream are taken from both
streams and are sorted by timestamp. You can do this with:

\begin{verbatim}
temps_and_hums = temp_readings
                   .outerJoin(hum_readings,
                              [:room_id, :room_id], :timestamp)
\end{verbatim}

Which results in:
\pagebreak
\begin{evaluation}
ezql> temps_and_hums
[\{ :room_id => "A", :temp =>   20, :hum => null \} at 11:00:00,
 \{ :room_id => "A", :temp => null, :hum => 0.62 \} at 11:00:30,
 \{ :room_id => "B", :temp => null, :hum => null \} at 11:01:00,
 \{ :room_id => "A", :temp => null, :hum => 0.62 \} at 11:01:30,
 \{ :room_id => "A", :temp =>   23, :hum => null \} at 11:02:00,
 \{ :room_id => "A", :temp => null, :hum => 0.61 \} at 11:02:30,
 \{ :room_id => "B", :temp =>   22, :hum => 0.59 \} at 11:03:00]
\end{evaluation}

Unfortunately, the fact that some values are \verb=null= influences
the results of correlations between these fields and restricts the
usefulness of this operator. For instance, if we try to answer our
original question, ``what was the maximum temperature while the
humidity was above 60\%?'' with the events above, the answer would be
\verb=null= because, as far as the engine knows, every time the
humidity was above 60\%, the temperature was \verb=null=. In chapter
\ref{chap:continuous-values} we will see how to correctly answer the
previous question using objects and continuous values.

\subsection{Sliding windows}
\label{sec:sliding-windows}

Windows allow the developer to specify which parts of the stream he is
interested in. There are two types of sliding windows in EzQL:
temporal and fixed-size. Temporal windows select only the tuples
within a given timestamp. For example, to obtain the Apple tuples
received in the last 5 minutes, one could do:

\begin{verbatim}
recent_apple = apple_stocks[5 min]
\end{verbatim}

If this statement is evaluated at 11:05:00 am, \verb=recent_apple=
will contain:

\begin{evaluation}
ezql> recent_apple
[\{ :symbol => "AAPL", :price => 52 \} at 11:00:02 am,
 \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

Fixed-size windows, on the other hand, select tuples based on their
position. The following code selects the last 3 elements received in
the original \verb=stocks= stream:

\begin{verbatim}
last_3 = stocks[0 .. 2]
\end{verbatim}

Note that streams are 0-based. The previous example will result in:

\begin{evaluation}
ezql> last_3
[\{ :symbol => "MSFT", :price => 70 \} at 11:00:03 am,
 \{ :symbol => "MSFT", :price => 75 \} at 11:00:06 am,
 \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

Just like aggregators, windows are updated as time passes or new
events arrive.

In the future, it should also be possible to support more complex
definitions of windows. For instance, ``all events from the past
week'', ``all events from Monday up to yesterday'', ``the fifty latest
events from the last 10 minutes'', etc.

Windows are just special kinds of streams where events ``expire'' and
are removed. Thus, most stream operations are also available on
windows. One could, for instance, count the number of events received
per company over the last 5 seconds with:

\begin{verbatim}
count_per_company =
  stocks[5 sec].groupby(:symbol,
                        \group -> group.count())
\end{verbatim}

If this statement is evaluated at 11:00:08 am, it will result in the
following dictionary:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & count() \\
  \hline
  ``AAPL'' & 1 \\
  ``MSFT'' & 2 \\
  \hline
\end{tabular}

Note also that the statement is equivalent to:

\begin{verbatim}
  count_per_company =
    stocks.groupby(:symbol,
                   \group -> group[5 sec].count())
\end{verbatim}

That is, applying the window before or after the \emph{group by}
returns the same result.

%\subsection{The flux operator}
%\label{sec:flux}
%
%The flux (\verb=|>=) operator is basically just a replacement for the
%. (dot) operator that suggests the idea of a pipeline where tuples
%flow from operator to operator. For example, to filter all Apple
%tuples and convert the \verb=:price= from dollars to euros and then
%obtain the average, one could write:
%
%\begin{verbatim}
%stocks
%  .where  (\t -> t.symbol == "AAPL")
%  .select (\t -> { t with :price => dol2eur(t.price) })
%  .avg    (:price)
%\end{verbatim}
%
%With the flux operator, the same query could be written as:
%
%\begin{verbatim}
%stocks
%  |> where  (\t -> t.symbol == "AAPL")
%  |> select (\t -> { t with :price => dol2eur(t.price) })
%  |> avg    (:price)
%\end{verbatim}

\section{Light syntax}
\label{sec:light-syntax}

Up until now we have been using the operators as functions that
receive arguments --- which may themselves be other functions ---, and
that act on streams to generate other streams, dictionaries, windows
or continuous values.

EzQL supports an alternative syntax resembling SQL, henceforth called
\emph{light syntax}. A query that filters all Apple tuples and
converts the \verb=:price= from dollars to euros looks like this
written in the light syntax:

\begin{verbatim}
from   t in stocks
where  t.symbol == "AAPL"
select { t with :price => dol2eur(t.price) }
\end{verbatim}

The \verb=select= statement at the end may look out-of-place to SQL
developers. The reasoning behind this decision is simple: by declaring
variables in the \verb=from= statement before using them in the
\verb=select= clause, one may get advanced support from the editor,
like code-completion or IntelliSense. For instance, the development
environment could provide the developer with a list of event fields
when he types in ``\verb=t.='' in the previous example.

This light syntax is really just syntactic sugar: the compiler
translates code from the light to raw syntax before processing. Why
support both syntaxes if they are equivalent? First of all, the point
of raw syntax is to show that SQL-like operators can be supported
using simple procedures --- and remember, EzQL is a procedural
language by nature. More importantly than that, however, is that we
would like if, in the future, the developer could implement his own
operators, that is, his own version \verb=select=, \verb=where=,
\ldots or any new operator he needs that EzQL doesn't provide. That's
part of the \emph{be extensible} goal defined in section
\ref{sec:design-goals}. With light syntax, one needs to modify the
parser to accommodate these new operators, while with raw syntax one
just needs to implement the operator as a function and may use it
right away.

From now on we will use light syntax in all our examples.

\chapter{Modeling entities with objects}
\label{chap:objects}

\section{Motivation}

In section \ref{sec:timestamp-join} we were given two streams ---
\verb=temp_readings= and \verb=hum_readings= ---, containing
temperature and humidity readings from a set of rooms in a
building. We then failed to provide an answer to an apparently simple
question: ``what was the maximum temperature while the humidity was
above 60\%?''. Using a timestamped outer join seemed like the right
thing to do, but this approach failed because whenever the humidity
was greater than 60\%, the temperature was \verb=null=.

However, in this scenario, we are dealing with physical quantities
measured frequently. It wouldn't be completely unreasonable to ignore
\verb=null=s (after all they were created by the join) and assume that
temperatures and humidities retain their previous values between
events. This approach would work but, in general, we can't make these
kind of assumptions. Sometimes the inexistence of an event really
means that nothing has happened (orders not placed, earthquakes that
didn't happen, \ldots) so it's wrong to assume the previous value
remains the same in the general case. Besides, sometimes \verb=null=s
may actually be valid values and it's wrong to ignore them.

Joining streams has another weakness. In this example we only had to
deal with temperatures and humidity readings. But imagine we need to
take other kinds of sensors into account: atmospheric pressure, noise
level, \ldots. To use all these different readings in a query, we
would have to join them all. Furthermore, to perform calculations over
each room independently, we need an additional \emph{group by}. All
these \emph{join}s and \emph{group by}s difficult the understanding of
queries and hide their real intention, i.e., that there are rooms with
temperatures and humidity levels and we want to compute results for
each room individually based on those values.

There are two problems here. First, we have an entity --- the room
---, whose properties --- temperature and humidity ---, are scattered
around multiple streams and need to be merged together for further
processing. Having multiple rooms aggravates the issue, because the
properties of multiple entities become all mixed up. Second, events
aren't supposed to carry stateful information. Events represent
isolated occurrences. A temperature reading of 20 degrees means that
the temperature at that time was 20 degrees, but it doesn't say
anything about the evolution of temperature in the time that goes from
this event to the next. We can't just ``fill the gaps'' and we can't
assume ``one size fits all'' because, as said above, sometimes it
makes sense to assume some value remains constant between updates,
sometimes it doesn't.

What we really need to solve both problems is some way to gather all
the related information together and to explicitly manage stateful
attributes and how they evolve over time. What we really need is to be
able to model the scenario using objects.

\section{Simple objects}
\label{sec:simple-objects}

From an OOP perspective, it is clear that our scenario could be
modeled with many Room objects (one per each physical room) with
temperature and humidity attributes.

In EzQL, this could be accomplished with:

\begin{verbatim}
@createFrom(temp_readings, onNew = :room_id)
@createFrom(hum_readings,  onNew = :room_id)
class Room
end
\end{verbatim}

This code declares a new class --- \verb=Room= ---, but leaves the
declaration empty. However, is also tags the class with two
\verb=@createFrom= annotations. This signals the ESP engine to
automatically create new \verb=Room= instances when previously unseen
\verb=room_id=s show up in either \verb=temp_readings= or
\verb=hum_readings= streams.

In addition, this annotation automatically adds the stream fields to
the class. So, every Room instance ends up with three attributes:
\verb=room_id=, \verb=temp= and \verb=hum=. These attributes are
automatically filled and updated by the system as new events
arrive. Naturally, the values of these attributes are independent from
object to object. That is, an event with \verb=room_id= equal to 2
will only affect the Room object with \verb=room_id= equal to 2 and
not the others.

Having set up this apparatus, we can now run the simulation and see
the objects being created for us automatically. All the \verb=Room=
instances are accessible using \verb=Room.all=. In essence, this is a
dictionary that maps object id's (taken from the \verb=:room_id= field
in this case) to the objects themselves. If the stream
\verb=temps_readings= contains:

\begin{verbatim}
[{ :room_id => "A", :temp => 20 } at 11:00:00 am,
 { :room_id => "B", :temp => 21 } at 11:01:00 am,
 { :room_id => "A", :temp => 23 } at 11:02:00 am,
 { :room_id => "B", :temp => 22 } at 11:03:00 am]
\end{verbatim}

and if \verb=hum_readings= receives:

\begin{verbatim}
[{ :room_id => "B", :hum => 0.60 } at 11:00:30 am,
 { :room_id => "A", :hum => 0.62 } at 11:01:30 am,
 { :room_id => "A", :hum => 0.61 } at 11:02:30 am,
 { :room_id => "B", :hum => 0.59 } at 11:03:00 am]
\end{verbatim}

then \verb=Room.all= will return:

\begin{tabular}{ |l|l| }
  \hline
  id & object \\
  \hline
  ``A'' & Room with room\_id = ``A'', temp = 23, hum = 0.61 \\
  ``B'' & Room with room\_id = ``B'', temp = 22, hum = 0.59 \\
  \hline
\end{tabular}

Note that the temperature inside the room A is 23 degrees even though
the last update was received at least 1 minute ago. It's like in a
regular OOP programming language: if an attribute isn't modified, it
retains the previous value.

Naturally, we can query these objects as if they were normal streams:

\begin{verbatim}
from   room in Room.all
where  room.hum > 0.60
select room.room_id
\end{verbatim}

This would result in a subset of \verb=Room.all= containing only one
room:

\begin{tabular}{ |l|l| }
  \hline
  id & object \\
  \hline
  ``A'' & Room with room\_id = ``A'', temp = 23, hum = 0.61 \\
  \hline
\end{tabular}

Previously, we said that all \verb=Room= instances automatically get
three fields from the streams \verb=temp_readings= and
\verb=hum_readings=. This is the default behavior, as it will most
likely be the intention of the developer, but it may be overridden if
desired. Furthermore, one may create new fields whose values are taken
from an arbitrary stream.

TODO: Give example

\section{Associations}
\label{sec:associations}

Suppose now that you are trying to model a scenario where there is a
factory with many rooms. Products follow a predetermined path along
the pipeline and may switch rooms frequently. Every product has a
RFID tag and every room's entry has RFID readers which emit events to
the \verb=entries= stream declared with:

\begin{verbatim}
entries = new Stream of { :room_id, :product_id }
\end{verbatim}

The question you need to answer is simply ``What is the temperature at
the room where product X is?''.

This is not so difficult using regular stream operations:

\begin{verbatim}
product_to_room = entries
                    .groupby(:product_id,
                             \group -> group.last)

Room.all[product_to_room["X"]].temp

TODO: Explain last. Returns the latest event in a stream
\end{verbatim}

But if you need to know the number of products in each room, you'd
have to create the opposite relationship, \verb=room_to_product=:

\begin{verbatim}
room_to_product = product_to_room
                    .flatten
                    .groupby(:room_id,
                    \group -> group.count())

TODO: explain flatten. Basically, it turns a dictionary produced
by group by into a regular window:

if product_to_room =

product_id -> room_id
"X"        -> "G53"
"Y"        -> "A"
"Z"        -> "A"

product_to_room.flatten =

[{ :product_id => "X", :room_id => "G53" },
 { :product_id => "Y", :room_id => "A" },
 { :product_id => "Z", :room_id => "A" }]

But is it a regular window? I mean, can you do .flatten[5 min] ?

\end{verbatim}

Clearly, this works, but it would be much easier if our objects could
be enriched with associations to each other. In this case, we'd like
to add a list of \verb=Product=s to every \verb=Room= and a reference
to a \verb=Room= to every product. This can be done in EzQL as
follows:

\begin{verbatim}
@createFrom(temp_readings, onNew = :room_id)
@createFrom(hum_readings,  onNew = :room_id)
@hasMany(:products)
class Room
end

@createFrom(entries, onNew = :product_id)
@belongsTo(:room)
class Product
end
\end{verbatim}

The \verb=@hasMany= annotation means that each room may contain an
unbounded number of products. This annotation also adds a list of
\verb=Product= instances to every \verb=Room=. So, if you want to get
the list of products in room A you can do so with:

\begin{verbatim}
Room.all["A"].products
\end{verbatim}

The \verb=@belongsTo= annotation adds the reverse association, i.e.,
it means that every product is inside one room. Moreover, this
annotation also adds an attribute \verb=room= to every
\verb=Product=. Thus, to know in which room is product X one could
write:

\begin{verbatim}
Products.all["X"].room
# or ...
Products.all["X"].room.room_id  # ... to get just the ID
\end{verbatim}

And you can find the number of products per room using:

\begin{verbatim}
from   room in Room.all
select room.room_id, room.products.length
\end{verbatim}

Certainly an improvement. Not only is the query more concise, it is
also much simpler to understand.

All these attributes created automatically are managed by the engine
without any additional effort required from the programmer. All of
this may seem like a bit of hidden magic, but it's really just a
simple convention:

\begin{enumerate}
\item The \verb=:products= parameter to the \verb=@hasMany= annotation
  instructs the engine to create a list of \verb=Product= instances
  (it's simply the class whose name is the singular of
  ``products''). Furthermore, this list will be named ``products'';
\item On the other hand, the \verb=:room= parameter to the
  \verb=belongsTo= annotation tells the engine to create a field named
  ``room'' in every product. This field will hold an instance of the
  \verb=Room= class (once again, it finds the class name through the
  parameter);
\item How does the engine fill the \verb=room= attribute of every
  \verb=Product=? Easy, it checks the \verb=room_id= field. Remember,
  this field was created automatically by the \verb=@createFrom=
  annotation. Given the identifier of the room, the engine can easily
  lookup the corresponding \verb=Room= instance using the
  \verb=Room.all= dictionary;
\item Finally, after knowing where each product is, it is
  straightforward to invert the association to find out which products
  are in which rooms.
\end{enumerate}

Naturally, this convention may be overridden.

% TODO: give examples
% Which other relation types are supported? Give examples.

\chapter{Continuous values}
\label{chap:continuous-values}

Just as you may query windows containing old events, you may reference
old attribute values in your queries. For example, you can calculate
the maximum temperature of room A over the last 5 minutes using the
\verb=temp= attribute from the \verb=Room= class as follows:

\begin{verbatim}
Room.all["A"].temp[5 min].max()
\end{verbatim}

You could have solved this using standard stream operators presented
in chapter \ref{chap:streams-events}:

\begin{verbatim}
from   ev in temp_readings
where  ev.room_id = "A"
select max(ev.temp)
\end{verbatim}

Surprisingly though, the results would have been different. To
understand why, imagine the \verb=temp_readings= stream receives the
following events:

\begin{verbatim}
 [{ :room_id => "A", :temp => 25.0 } at 10:54:00 am,
  { :room_id => "A", :temp => 24.9 } at 10:57:00 am]
\end{verbatim}

Also, assume that between events the temperature remains constant. In
the real-world there are always small oscillations, but imagine the
sensor is prepared to ignore these and emit new results only when they
differ from the previous one by a given amount (0.1 degrees in this
case).

Now, suppose the question is posed at 11h am and no further updates
were received. What was the maximum temperature over the last 5
minutes? Intuitively, you would answer 25 degrees, because that was
the temperature 5 minutes before. However, the event that contains
that temperature is, as of 11h am, outside the window. So, using just
regular sliding windows on streams will return 24.9 --- the wrong
result.

Objects, on the other hand, behave differently. As mentioned in
section \ref{sec:simple-objects}, an object's attribute retains its
value until a new event changes it. One could plot the evolution of an
attribute over time as a step function that changes when new events
arrive. Figure [something] illustrates this for the
\verb=Room.all["A"].temp= attribute. As the plot shows, the
temperature at 10:54:00 am became 25 degrees and remained there until
10:57:00 am, well inside the 5 minutes window. Thus, querying the
attribute for its maximum value over the last 5 minutes would return
25.

[TODO Picture]

This is surprising because objects were introduced with the intention
of simplifying queries. But objects also introduce state and this
seemingly innocuous change has profound implications. In essence,
events are discrete while attributes are continuous --- i.e., their
value is always defined. With objects, writing many state related
queries --- like the one in this example --- becomes easier than using
only regular stream operations.

Attributes belong to a broader class of expressions called
\emph{continuous values}, which we may define as expressions whose
past history is well known. This past history includes not only the
past values the expressions evaluated to, but also the time interval
in which they did.\footnote{Keeping all the past values at the hands
  of the developer may seem unrealizable because its simply too much
  data. However, please note that the runtime should be able to
  determine which data in the past history won't be needed any longer
  and delete it. How to implement this is outside the scope of this
  document.} Continuous values provide an elegant way to solve many
time related queries, as we will see next.

\section{Aggregators over continuous values}

The fact that continuous values retain their previous values allows
one to compute aggregations over those values as we saw in the
previous section. However, continuous values hold two dimensions of
information: their previous values and the time intervals those values
hold. The traditional aggregations we are used to --- \verb=max=,
\verb=avg=, \verb=sum=, \ldots ---, only care for the previous
values. But we can exploit the time intervals information to create
and experiment with new kinds of aggregators. For example, imagine you
want to know for how long was the temperature equal to 25 degrees. We
could create a \verb=sumIntervalsWhereValueIs= aggregator that checks
the past history of the temperature and sums the intervals where it
was equal to some parameter. Then, answering the question would be as
simple as:

\begin{verbatim}
Room.all["A"].temp.sumIntervalsWhereValueIs(25)
\end{verbatim}

Or you could find the biggest interval where the temperature was 25
degrees using another aggregator,
\verb=lengthOfMaxIntervalWhereValueIs=:

\begin{verbatim}
Room.all["A"].temp.lengthOfMaxIntervalWhereValueIs(25)
\end{verbatim}

These aggregators have rather lengthy names and they have a few
limitations. For instance, they only consider intervals with the same
value. What if you need to know for how many minutes was the
temperature above or below a certain threshold?

\section{Continuous queries produce continuous values}

Suppose you need to know if the current temperature is above or equal
to 25 degrees:

\begin{verbatim}
Room.all["A"].temp >= 25
\end{verbatim}

This expression it will return either \verb=true= or \verb=false=. But
what happens if the expression is used as part of a continuous query?
continuous query is always running and producing results. Hence, the
query should produce \verb=true= when the temperature reaches 25
degrees and \verb=false= when it decreases below 25. That is, in the
continuous context, the entire expression has a value every time the
continuous attribute, \verb=Room.all["A"].temp=, is defined. The
following table shows this correspondence:

\begin{tabular}{ |l|r|r| }
  \hline
  Time interval & \verb=Room.all["A"].temp= & \verb!Room.all["A"].temp >= 25! \\
  \hline
  $[$10:50; 10:53) & 25.0 & \verb=true=  \\
  $[$10:53; 10:55) & 24.9 & \verb=false= \\
  $[$10:55; 10:56) & 24.8 & \verb=false= \\
  $[$10:57; 10:59) & 24.9 & \verb=false= \\
  $[$10:59; 11:00) & 25.0 & \verb=true=  \\
  \hline
\end{tabular}

The boolean expression has a current value as well as a collection of
previous values. By definition then, the boolean expression is,
itself, a continuous value. So we can just use new aggregators like
the ones defined in the previous section to perform arbitrary
computations on these values. For example, to sum the time intervals
where the temperature was equal or above 25 degrees, you could use the
\verb=sumIntervals= aggregator, which sums up the intervals where the
value is \verb=true=:

\begin{verbatim}
(Room.all["A"].temp >= 25).sumIntervals()
\end{verbatim}

Note that to find out for how long was the temperature equal to 25
degrees, you could now write:

\begin{verbatim}
(Room.all["A"].temp == 25).sumIntervals()
\end{verbatim}

% TODO mention that this is done incrementally, unlike temporal
% databases which are ad-hoc

This is certainly more understandable than the
\verb=sumIntervalsWhereValueIs= aggregator defined in the previous
section.

\section{Composite attributes}

Let's go back to the ``products inside rooms'' example of section
\ref{sec:associations}. Suppose we have the following sequence of
events arriving into the \verb=entries= stream:

\begin{verbatim}
[{ :product_id => "X", :room_id => "A" } at 10:00 am,
 { :product_id => "X", :room_id => "B" } at 10:10 am
 { :product_id => "X", :room_id => "C" } at 10:17 am]
\end{verbatim}

That is, the product X enters room A at 10 am, 10 minutes later it
enters room B and finally, at 10:17 am it enters room C. After all
these events are received, the attribute
\verb=Product.all["X"].room_id= will be a continuous value with the
following history:

\begin{tabular}{ |l|r| }
  \hline
  Time interval & \verb=Product.all["X"].room_id= \\
  \hline
  $[$10:00; 10:10) & ``A'' \\
  $[$10:10; 10:17) & ``B'' \\
  $[$10:17;   now) & ``C'' \\
  \hline
\end{tabular}

Suppose we enrich our \verb=Room= instances with a temperature
attribute which gets its value from the temperature of the room the
product is in:

\begin{verbatim}
@createFrom(entries, onNew = :product_id)
@belongsTo(:room)
class Product
    temperature = this.room.temp
end
\end{verbatim}

As in Java or C++, \verb=this= is a reference to the current product
whose temperature is being accessed.

Finally, assume the following events are received in the
\verb=temp_readings= stream:

\begin{verbatim}
 [{ :room_id => "A", :temp => 10 } at 09:50:00 am,
  { :room_id => "B", :temp => 15 } at 10:09:00 am,
  { :room_id => "C", :temp => 20 } at 10:16:00 am]
\end{verbatim}

If we correlate \verb=entries= and \verb=temp_readings=, we see that
the temperature in room A was 10 degrees while the product X was
there, the temperature in room B was 15 degrees while the product was
there and finally, the temperature in room C was 20 degrees while the
product was there. Since the temperature of a product was defined as
the temperature of the room the product is in, the \verb=temperature=
attribute for product X will contain the following history:

\begin{tabular}{ |l|r|r| }
  \hline
 Time interval & Product X temperature \\
  \hline
  $[$10:00; 10:10) & 10 \\
  $[$10:10; 10:17) & 15 \\
  $[$10:17;   now) & 20 \\
  \hline
\end{tabular}

Now, you could, for instance, calculate the amount of time the
product's temperature was below 15 degrees with:

\begin{verbatim}
(Product.all["X"].temperature < 15).sumIntervals()
\end{verbatim}

Even though the current temperature of the product equals the current
temperature of its room, the history of temperatures of the product
does not equal the history of temperatures of its \emph{current}
room. The engine knows that the product switches rooms and is smart
enough to obtain the temperatures from the correct places. Note that
this behavior is embodied in the semantics of the \verb=.= (dot)
operator used in the definition of the \verb=temp_readings=
attribute. If you want, you can still access the history of
temperatures of the current room with a simple window:

\begin{verbatim}
Product.all["X"].room[now].temp
\end{verbatim}

\section{Aggregators as continuous values}
\label{sec:aggregators-continuous-values}

In section \ref{sec:aggregators} we alluded to the possibility of
considering an aggregator as a stream with all its past values. This
would allow one to, for instance, calculate the maximum price average
over the last 5 minutes of stock trades data. Unfortunately, we
concluded that treating aggregators as streams was not enough to solve
a few interesting problems such as ``For how long was the average
above a certain threshold?''. Actually, we could reuse the same
arguments employed in the beginning of this chapter to show that, with
streams, it wouldn't even be possible to know \emph{if} the average
was above the threshold at all.

These problems go away if we treat aggregators not as streams but as
continuous values instead. If you think a little about it, it makes
sense. An aggregator has a well defined value for a certain period of
time and it may be useful to consider its past values.

Using the \verb=sumIntervals= aggregator introduced in the previous
section, it is straightforward to find out for how long was the
average above the threshold. Assuming the \verb=apple_stocks= stream
contains the Apple stock data we want to analyze, we could write:

\begin{verbatim}
(apple_stocks.avg(:price) > THRESHOLD).sumIntervals()
\end{verbatim}

To conclude this chapter, we will go back to the ``rooms and
products'' example to develop one last query. The problem, attributed
to Andrew Witkowski from Oracle, is simple: what were the products
that spent at least 10 (consecutive or not) minutes at more than 20
degrees of temperature and 80\% of humidity, while inside the pipeline
(those products are spoiled and must be discarded)? The complete
solution in EzQL is transcribed below:

\begin{verbatim}
# Declare the streams
temp_readings = new Stream of { :room_id, :temp }
hum_readings  = new Stream of { :room_id, :hum }
entries       = new Stream of { :room_id, :product_id }

@createFrom(temp_readings, onNew = :room_id)
@createFrom(hum_readings,  onNew = :room_id)
@hasMany(:products)
class Room
end

@createFrom(entries, onNew = :product_id)
@belongsTo(:room)
class Product
    temperature = this.room.temp
    humidity    = this.room.hum
end

# Spoiled products spent at least 10 minutes at more than 20
# degrees and 60% of humidity
spoiled_products =
  from   prod in Product.all
  where  (prod.temperature > 20 and prod.humidity > 80)
           .sumIntervals() >= 10 min
  select prod
\end{verbatim}

\chapter{Batch mode}

Up until now we have been developing continuous queries, whose results
are updated as soon as new events arrive. However, sometimes it is
undesirable to compute all results incrementally due to efficiency
concerns. For example, calculating a standard deviation eagerly, as
new events arrive, is extremelly inefficient because every time a new
event arrives, the average may change and the standard deviation will
have to be recomputed again from the beginning. Yet, in other
scenarios it might even be impossible to calculate results
continuously because the events themselves decide the parameters of
the computation. An example may be the determination of the average
price of the last N stock events, where N is a variable given by
another event. Only after this event is received do you know which
events to consider in the operation.

EzQL supports a different mode of operation where results are computed
on demand. This mode is called \emph{batch mode}. You may enter batch
mode using the \verb=when= construct. The following example shows how
to calculate the standard deviation of the last 5 minutes of Apple
stocks when an event is received on the \verb=ReqStdDev= stream:

\begin{verbatim}
when ReqStdDev()
  apple_stocks[5 min].stddev(:price)
\end{verbatim}

Suppose now you want to generalize the previous example to support any
company, not just Apple. You could insert the symbol of the company in
a field of the \verb=ReqStdDev= event and use it as follows:

\begin{verbatim}
when ReqStdDev(?symbol):
  from  ev in stocks[5 min]
  where ev.symbol == ?symbol
  select stddev(price)
\end{verbatim}

In this example, the name of the company in the request is assigned to
the \verb=?symbol= variable that is then used to obtain the correct
events from the stream.

\verb=when= also supports guards that you may use to refine your
selection. For example, to consider only those companies whose symbol
begins with an 'A', you could write:

\begin{verbatim}
when ReqStdDev(?symbol)
  if ?symbol.startsWith("A")
    from  ev in stocks[5 min]
    where ev.symbol == ?symbol
    select stddev(price)
\end{verbatim}

Finally, suppose the request also includes a parameter N with the
number of events to use in the computation. N could be anything
between 0 (consider only the last event) to 99 (consider the last 100
events). It is necessary to restrict the range of values of N,
otherwise an infinite amount of space may be required to keep the
entire stream in memory. The solution in EzQL is transcribed below:

\begin{verbatim}
when ReqStdDev(?symbol, ?count)
  if ?symbol.startsWith("A") and ?count < 100
    from  ev in stocks[0 .. ?count]
    where ev.symbol == ?symbol
    select stddev(price)
\end{verbatim}


TODO

Some things are forbidden inside the batch mode, because the compiler
is unable to decide which data is relevant and which is not:

\begin{itemize}
\item Streams without windows (what about aggregators like avg or count?)
\item Semantic windows? Maybe not...
\end{itemize}

\chapter{Semantic windows}

Semantic windows are just like regular windows in that they allow the
developer to operate over part of a stream. However, unlike sliding
windows that are defined by size or time, semantic windows have their
endpoints fixed by events. For instance, imagine you need to know how
many products entered some room while product X whas there. This could
be solved with semantic windows as follows:

\begin{verbatim}
from entries(?room_id, ?product_id) until entries(?room_id, ?product_id)
  from entry in entries
  where entry.room_id == ?room_id
  select count()
\end{verbatim}


\begin{verbatim}
TODO Could this be solved using continuous values?

# How many products entered some room while the temperature of product X was > 20?

entry_count =
  for prod in Product.all
    while prod.temp > 20
      yield entries.count()

ezql> entry_count["X"]
3

# This I might need to do with the result:
# - Does the interval need to be consecutive or not?
# - If it does, how do I get the number of entries for each interval?
#
# Is this like a continuous value of the form:
#
# +--------------------------------+
# | Period the temp > 20 | count() |
# |----------------------+---------|
# | 11:00 .. 12:00       |      10 |
# | 13:30 .. 13:40       |      20 |
# | otherwise            |    null |
# +--------------------------------+
\end{verbatim}

Note that events inside sematic windows never expire, which allows a
few optimizations as some results may be computed incrementally and
not only when the window is closed. If this is possible, the events
may be discarded after a new intermediate result is calculated.

\chapter{User defined functions and aggregators}
\label{chap:udfs}

\begin{itemize}
\item Explain the need for new aggregators (sumIntervals());
\item The same definition must work for both continuous and batch modes;
\item Explain the aggregates;
\item Some aggregators cannot be used in continuous mode (e.g., stddev)
\item Some condition may depend only on time to become true
  (blah.sumIntervals() > 10 min). Should we provide some way to signal
  the engine to reevaluate the condition when it is supposed to become
  true?
\end{itemize}
\chapter{Reference}

\section{Stream}

\section{Window}

\section{Dictionary}

\section{Continuous values}

\chapter{Bibliography}
\end{document}
