\documentclass{report}

\usepackage{framed}
\usepackage{alltt}

\newcommand{\tuple}[1]{$(#1)$}

\newenvironment{evaluation}
{
  \framed
  \begin{alltt}
}
{
  \end{alltt}
  \endframed
}

\begin{document}
\title{Programming in EzQL: a tutorial}
\author{Lu\'{\i}s Pureza}

\maketitle

\tableofcontents

\addtolength{\parskip}{\baselineskip}
\chapter{Introduction}
\label{chap:introduction}
EzQL is a new programming language targeted at event stream processing
(ESP) applications. As explained in some detail in [ref], currently
available ESP languages lack expressiveness, making the task of writing
simple and reasonable queries rather cumbersome. We believe one of the
main reasons this happens is because the semantics of constructs
provided are weaker than the semantics that would be necessary to
answer those queries. That is, there is an impedance mismatch between
what the existing languages provide and what the developers really
need. EzQL is our proposal to overcome these limitations and enrich
ESP query languages with a new set of constructs that promises to
fulfill the needs of developers.

The main features of EzQL are:

\begin{itemize}
\item Combines the \emph{imperative} (C, Java) and the
  \emph{declarative} (SQL) paradigms allowing the developer to query
  data with a SQL-like language or by writing an algorithm, specified
  as a sequence of small steps, for those cases where SQL is not
  appropriate;
\item Includes a simple \emph{object-model} where real-world entities
  can be modeled as objects with properties --- temperature, price,
  speed, \ldots ---, whose values depend on the values of events. For
  instance, this object-model allows the creation of many Room objects
  each with their own temperature attribute which is automatically
  updated as new sensor readings arrive. Furthermore, these objects
  and the values of their properties define the state of the
  application and this state can be queried just like regular events;
\item The object model supports \emph{associations} between objects,
  inspired by O/RM (Object-Relational Mapping) tools for
  databases. Thus, it is possible to say, for instance, that a Room
  has many Products and, correspondingly, that a Product belongs to
  one Room. Again, these associations may change as new events arrive,
  i.e., a Product may leave one Room and enter another. Naturally,
  these relationships can also be queried --- one can ask, for
  example, which is the cheapest Product inside Room A. This type of
  queries can already be written in currently available systems, but
  we will see that they become much simpler in EzQL;
\item EzQL provides extensive support for \emph{temporal queries}, a
  common class of queries in ESP applications that current products
  don't handle so well, as discussed in [ref]. This support integrates
  with the object-model so that one can easily ask, for example, for
  how long was the temperature in some room above 20$^{\circ}$C during
  the last 10 minutes;
\end{itemize}

The purpose of this document is to describe EzQL in detail, so that
developers can understand its syntax and semantics and start using it
right away to solve new problems.

The rest of this chapter discusses the rationale behind the design of
EzQL and some of problems that are still unsolved. Then, the document
proceeds with chapter \ref{chap:streams-events} that explains what
exactly are streams and events and what can you do with them,
including how to query them. Chapter \ref{chap:object-model} describes
how to model real-world entities as objects in EzQL and how to query
their state and finally, the topic of chapter \ref{chap:udfs} is how
to extend the core set of operations provided by the EzQL environment
by writing your own functions and aggregators.

\section{Design goals}
\label{sec:design-goals}

Any new programming language such as EzQL must be designed with a set
of goals in mind. Some of these are quite general --- ``Be useful!'',
while others are chosen carefully by the designers to set the new
language apart from any other, like Python's ``There should be one ---
and preferably only one --- obvious way to do it''. These principles
become a major influence in the resulting language and even define
what developers sometimes call the philosophy of the language. It is
important that the language adheres to these principles. Otherwise, it
just won't make sense.

These are the goals that drove the design of the EzQL language:

\begin{description}
\item[Be powerful] The set of operations allowed in a query should be
  large enough to accommodate all reasonable things users might want
  to do with their data. Basically, frequent operations should be
  simple and rare operations should be possible;
\item[Be extensible] ESP is a novel area with unique requirements that
  are still being discovered. No one really knows the boundary between
  what is ESP and what is not, but everyday ESP systems are applied to
  new problems with their own characteristics. It is mandatory that
  the language be extensible enough to accommodate this evolution,
  much as general purpose languages adapt to new domains through the
  development of new libraries;
\item[Be expressive] EzQL queries should be straightforward to read,
  write and understand by the developer. In particular, it should be
  possible to partition big queries into smaller ones in a
  step-by-step fashion --- like a procedural programming language,
  where each statement specifies a new operation acting on the result
  of the previous statement. Also, common operations should be
  expressible directly by what they mean and not as an opaque set of
  SQL-like constructs that hide their real meaning;
\item[Be coherent] In Smalltalk, everything is an object and even the
  addition of two numbers is seen as messages flowing between
  them. Lisp programs are just data and any Lisp fan will happily
  explain why that has so many advantages. Java, on the other hand,
  distinguishes between built-in types like integers and user-defined
  types and this distinction frequently bothers developers. Coherency
  makes the language easier to understand because you have less rules
  and exceptions. But it's far more than that: a coherent language
  shows how well-thought it was. This kind of elegance cannot be
  ignored.
\end{description}

% One major source of inspiration during the design of EzQL was
% Microsoft's LINQ [ref]. LINQ --- Language INtegrated Query ---, is a
% library for the .NET platform that provides a SQL-like language to
% query not only databases, but also in-memory collections like lists or
% dictionaries, XML files and even online services like Amazon
% store. LINQ for databases includes an O/RM module that creates objects
% from database entities and manages their properties and
% relationships. These objects may, of course, be used in normal LINQ
% queries just like raw entities, something we want EzQL be able to do
% as well. Furthermore, note that LINQ comes with its own SQL-like query
% language, but this language is expected to be used in conjunction with
% other .NET languages, like C# or VB.NET. This way, the developer may
% choose to use LINQ for all his data processing tasks or he may opt to
% use LINQ for some tasks and rely on good old imperative constructs to
% implement more complex operations, which is also one of the main
% characteristics of EzQL. In fact, the similarities between LINQ and
% EzQL are so many that it could be said that EzQL is in essence a LINQ
% for ESP.

%http://msdn.microsoft.com/en-us/library/bb425822.aspx#linqtosql_topic5
% [Table(Name="Orders")]
% public class Order
% {
%    [Column(Id=true)]
%    public int OrderID;
%    [Column]
%    public string CustomerID;
%    private EntityRef<Customer> _Customer;    
%    [Association(Storage="_Customer", ThisKey="CustomerID")]
%    public Customer Customer {
%       get { return this._Customer.Entity; }
%       set { this._Customer.Entity = value; }
%    }
% }

% var q =
%    from c in db.Customers
%    from o in c.Orders
%    where c.City == "London"
%    select new { c, o };



By the end of this document, EzQL should just make sense. Every
feature or construct should have a well-defined purpose and be
generally applicable to a wide-range of ESP problems.

\section{Unsolved problems}
\label{sec:unsolved-problems}

These are some of the current limitations of EzQL:

\begin{itemize}
\item No pattern matching to detect complex patterns of events. In the
  future, one will probably integrate a pattern language such as SASE+
  [ref] directly into EzQL, much like regular expression libraries
  provide one mini-language inside another;
\item It doesn't integrate with databases. This is a serious drawback
  because not all information is available in events. For instance,
  when a costumer places an order, an event may be generated for the
  system to process. But what if the application needs to know the
  name of the costumer? Should it be part of the event? What about the
  address? What about the previous orders the costumer placed?
  Clearly, this information should already be available in the
  database and thus it is undesirable to enlarge the events with
  redundant information. Nonetheless, currently EzQL does not address
  this issue;
\item Out-of-order events: still an ongoing research topic.
\end{itemize}

\chapter{Streams and events}
\label{chap:streams-events}

An event signals that something happened. Exactly what happened is
encoded in the event itself as a list of field names packed together
with their corresponding values. Moreover, all events have an implicit
timestamp for when the event occurred. For example, an event signaling
that Apple stocks were priced at \$50 at 10:59:59 am today, may be
represented with the following encoding:

\begin{verbatim}
  { :symbol => "AAPL", :price => 50 } at 01/11/2009 10:59:59 am
\end{verbatim}

This event includes two fields: \verb=:symbol= with value ``AAPL'' and
\verb=:price=, with value ``50''. Field names are always prefixed with
a colon. From now own we will assume all events happened today and
will omit the date from their timestamp when we print them.

To access an event's field one uses the . (dot) operator:

\begin{evaluation}
  ezql> event = \{ :symbol => "AAPL", :price => 50 \}
  ezql> event.symbol
  "AAPL"
  ezql> event.price
  50
  ezql> event.timestamp
  <the date and time the event was created in the first line>
\end{evaluation}

As can be seen in the example, the timestamp is available as if it
were a normal field.

A stream is an abstract data type that collects events. Informally, a
stream could be seen as a list of events ordered by their timestamp,
i.e., older events at one end, newer events at the other end. However,
this is not entirely accurate because streams are active entities and
possibly infinite. For instance, a stream that collects events from
financial markets may receive thousands of new events every second
and, what's more important, this flow of events may never terminate.

\section{Query operations}
\label{sec:query-operations}
The query operations include all the SQL-like constructs that can be
used to manipulate data.

The result of each operation will be illustrated with examples of
applications over a stream with stock data where each tuple contains
two fields, \verb=:symbol= and :price. It is assumed that this stream,
\verb=stocks=, contains the following tuples:

\begin{verbatim}
 [{ :symbol => "AAPL", :price => 50 } at 10:59:59 am,
  { :symbol => "AAPL", :price => 52 } at 11:00:02 am,
  { :symbol => "MSFT", :price => 70 } at 11:00:03 am,
  { :symbol => "MSFT", :price => 75 } at 11:00:06 am,
  { :symbol => "AAPL", :price => 50 } at 11:00:07 am]
\end{verbatim}

Naturally, these values are only for illustration purposes and are not
representative of the real market.

\subsection{Projections}
\label{sec:projections}
A projection applies a function to all tuples in a stream and returns
a new stream with the results. For instance, if \emph{stocks} is a
stream with tuples of the form \tuple{symbol, price} representing the
price of stock actions, one could double the \emph{price} with:

\begin{verbatim}
  price2x = stocks
              .select (\t -> { :symbol => t.symbol,
                               :price => t.price * 2 })
\end{verbatim}

The first thing to clarify about this example is that the piece of
code inside parenthesis, \verb!\t -> { ... }! is actually an anonymous
or lambda function that receives one argument, \verb=t= and produces a
new tuple with fields \verb=:symbol=, copied from \verb=t=, and
\verb=:price= which is equal to twice the price of the
argument. \verb=select= receives this anonymous function, calls it for
every tuple inside the \verb=stocks= stream and saves the results in a
new stream, which it then returns. \verb=select= is really just a
continuous version of LISP's \verb=mapcar= or Python's \verb=map=.

The result of the previous expression is:

\begin{evaluation}
  ezql> price2x
  [\{ :symbol => "AAPL", :price => 100 \} at 10:59:59 am,
   \{ :symbol => "AAPL", :price => 104 \} at 11:00:02 am,
   \{ :symbol => "MSFT", :price => 140 \} at 11:00:03 am,
   \{ :symbol => "MSFT", :price => 150 \} at 11:00:06 am,
   \{ :symbol => "AAPL", :price => 100 \} at 11:00:07 am]
\end{evaluation}

One can perform further operations on this stream, like
converting the price from euros to dollars:

\begin{verbatim}
  price2x_dol = price2x
                  .select (\t -> { :symbol => t.symbol,
                                   :price => eur2dol(t.price) })
\end{verbatim}

Note that in the previous examples you're just copying \verb=:symbol=
around. This is OK for one field, but could get boring pretty quickly
if the number of fields increases. An alternative syntax might come in
handy for these situations:

\begin{verbatim}
  price2x_dol = price2x
                  .select (\t -> { t with
                                     :price => eur2dol(t.price) })
\end{verbatim}

This should read as ``Copy all the fields from \verb=t= except
\verb=:price=, which should be calculated as follows''.

\subsection{Filters}
\label{sec:filters}

One can also filter tuples in a stream based on a predicate. The
resulting stream will include only those tuples that pass the
predicate. For example, to obtain only the Apple tuples one could
write:

\begin{verbatim}
  apple_stocks = stocks
                   .where (\t -> t.symbol == "AAPL")
\end{verbatim}

Note that the predicate is a function that receives an event and
returns a boolean. Like \verb=select=, \verb=where= calls the
predicate for every tuple but adds to the resulting stream only those
tuples where the predicate returns true.

Evaluating the stream \verb=apple_stocks= results in:

\begin{evaluation}
  ezql> apple_stocks
  [\{ :symbol => "AAPL", :price => 50 \} at 10:59:59 am,
   \{ :symbol => "AAPL", :price => 52 \} at 11:00:02 am,
   \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

At this point, it is convenient to pause and reflect a little about
the execution model of EzQL programs. Imagine the following query,
that doubles the price of Apple events:

\begin{verbatim}
  apple_stocks_2x =
    stocks
      .where  (\t -> t.symbol == "AAPL")
      .select (\t -> { t with :price => t.price * 2 }
\end{verbatim}

Unlike regular programming languages, EzQL statements like the
previous one are not executed sequentially. This query should not read
as ``Take all events in the \verb=stocks= stream, then filter out
those that do not belong to Apple and finally double the price of the
remaining stocks''. This is because, as described in the beginning of
the chapter, streams receive new events constantly. So you can never
say ``Take all events in \verb=stocks= stream'', because this stream
will never be complete. Instead, we must operate on events as they
arrive. Thus, what the previous query really means is: ``As new events
arrive in the \verb=stocks= stream, ignore those not related to Apple,
double the price of the others and put them in the
\verb=apple_stocks_2x= stream''. The difference is subtle but
important. Essentially, it means the previous query is always
running. To see this, imagine you evaluate \verb=apple_stocks= at
10:59:00 am:

\begin{evaluation}
  ezql> apple_stocks_2x
  []
\end{evaluation}

As expected, the stream is empty. But if you evaluate it again 5
minutes later, at 11:04:00 am, the stream won't be empty anymore:

\begin{evaluation}
  ezql> apple_stocks_2x
  [\{ :symbol => "AAPL", :price => 100 \} at 10:59:59 am,
   \{ :symbol => "AAPL", :price => 104 \} at 11:00:02 am,
   \{ :symbol => "AAPL", :price => 100 \} at 11:00:07 am]
\end{evaluation}

Note that you didn't have to recompute the entire
\verb=stocks.where=\ldots expression.

\subsection{Aggregators}
\label{sec:aggregators}

Aggregators are functions that are applied to streams and return a
single value. Example aggregators include \verb=min()=, \verb=max()=,
\verb=sum()= or \verb=avg()=.

For example, the average price attained by Apple stocks can be
found out with:

\begin{verbatim}
  avg_apple = apple_stocks.avg(:price)
\end{verbatim}

In this example, \verb=avg_apple= is not a standard numeric
value. Instead, it is a \emph{continuous} value that will change over
time, as new tuples arrive. Thus, if at any time you need to print the
average of Apple's stock prices, you need only to print
\verb=avg_apple= and not the entire expression on the right side.

The fact that the value of the aggregator changes, suggests that it
may be seen as stream with all its previous values. But new averages
are not really new events and the analogy breaks easily. For example,
if the average of Apple stocks was a stream, one could filter this
stream to obtain the averages greater than a given threshold:

\begin{verbatim}
  avg_apple
    .where (\avg -> avg.value > THRESHOLD)
\end{verbatim}

But what if the developer needs to find out for how long was the
average greater than the given threshold? Unfortunately, this cannot
be easily answered if we treat new averages as events, because events
``happen'' at a well-defined time, but provide no information
regarding their duration. Without the lower averages, we lose
continuity and can't know for sure for how long was a given average
valid, because the one that followed it may not be in the stream
anymore. Thus, treating aggregators as streams is a weak analogy.

What we would really like to have is a two dimensional plot of the
evolution of the aggregator's value over time. This way, answering the
question would be a simple matter of cutting the plot in two by the
threshold and sum the lengths of the time intervals where the value
was above the cut. This plot is a \emph{continuous value}, a new kind
of data abstraction for values that are truly continuous, i.e., their
value is defined even in the absence of events. Continuous values and
their operations will be discussed in section [something], but for
now, here's how we could find out for how long was the average greater
than the threshold:

\begin{verbatim}
  (avg_apple > THRESHOLD).sumIntervals()
\end{verbatim}

\subsection{Sorting}
\label{sec:sorting}

TODO

Sorting may only be applied to Windows, not streams.

\subsection{Group by}
\label{sec:group-by}

In its simplest form, \emph{group by} acts as demultiplexer
partitioning a single stream into many smaller streams. Events are
sent to one of these streams where an aggregator computes some value
per stream.

For example, to find the maximum stock price for each company one
could write:

\begin{verbatim}
  max_by_company = stocks
                     .groupby(:symbol,
                             (\group -> group.max(:price)))
\end{verbatim}

The second parameter received by \emph{group by} is, once again, a
higher-order function that receives a group and specifies what to do
with that group.

The result of this particular \emph{group by} is a dictionary that
maps symbols to maximums:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & Maximum price \\
  \hline
  ``AAPL'' & 52 \\
  ``MSFT'' & 75 \\
  \hline
\end{tabular}

The maximum price achieved by Apple could be discovered writing
simply:

\begin{evaluation}
  ezql> max_by_company["AAPL"]
  52
\end{evaluation}

Once again, please note that this maximum is actually a continuous
value that may change as new events arrive.

The \emph{group by} operator also allows partitioning by more than one
field at a time:

\begin{verbatim}
  price_count_per_company = stocks
                              .groupby({ :symbol, :price }
                                      (\group -> group.count()))
\end{verbatim}

Which results in:

\begin{tabular}{ |l|r|c| }
  \hline
  :symbol & :price & count() \\
  \hline
  ``AAPL'' & 50 & 2 \\
  ``AAPL'' & 52 & 1 \\
  ``MSFT'' & 70 & 1 \\
  ``MSFT'' & 75 & 1 \\
  \hline
\end{tabular}

The following query accesses this dictionary to find out how many
times Apple stocks were priced at exactly \$50:

\begin{evaluation}
  ezql> price_count_per_company[\{ :symbol => "AAPL",
                                  :price => 50 \}]
  2
\end{evaluation}

Finally, one can also specify arbitrary partitions:

\begin{verbatim}
  hourly_price_per_company =
    stocks
      .groupby(\t -> { :symbol, :hour => t.timestamp.hour() },
              (\group -> group.max()))
\end{verbatim}

The previous example calculates the maximum price per hour. The first
parameter is a function that, when given an event, chooses the group
in which it is going to put the event. This results in:

\begin{tabular}{ |l|r|c| }
  \hline
  :symbol & :hour & max() \\
  \hline
  ``AAPL'' & 10 & 50 \\
  ``AAPL'' & 11 & 52 \\
  ``MSFT'' & 11 & 75 \\
  \hline
\end{tabular}

Once again, accessing the results is pretty straightforward:

\begin{evaluation}
  ezql> hourly_price_per_company[\{ :symbol => "MSFT",
                                    :hour   => 11 \}]
  75
\end{evaluation}

Despite not being exactly a stream, some stream operations may be
applied onto the result of a \emph{group by}. In particular,
\verb=where= returns a new dictionary with only the groups that pass a
given predicate. For example, the following query returns a dictionary
containing only the companies whose maximum price is above \$70:

\begin{verbatim}
  max_by_company.where (\k, v -> v > 70)
\end{verbatim}

In this example the predicate is a function that receives two
parameters, \verb=k= and \verb=v=, which stand for the groups' key and
value in the \verb=max_by_companies= dictionary. Note that
\verb=where= when applied to the result of a \emph{group by} acts just
like the \verb=HAVING= operator in SQL.

This following dictionary is the result of the operation:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & Maximum price \\
  \hline
  ``MSFT'' & 75 \\
  \hline
\end{tabular}



TODO: Currently there must be an aggregator at the end of the line. Is
this reasonable? What if we want to return, I don't know, the latest 3
events per group? It should be perfectly acceptable to do so.

\subsection{Joining multiple streams}
\label{sec:join}

TODO - not really that important with the objects model.

Types of joins:
\begin{itemize}
\item equi-join
\item Cross join
\item Left and right outer joins
\item Full outer join
\end{itemize}

\subsubsection{Joining with timestamps}
\label{sec:timestamp-join}

There is a situation where you may join two streams without using
windows: when the timestamp is part of the join predicate. Let's see a
simple example to understand how these kinds of joins work.

To vary a little, assume now that we have two different streams,
\verb=temperatures= and \verb=humidity=, that contain temperature and
humidity readings for rooms in a building. \verb=temperature= contains:

\begin{verbatim}
 [{ :room_id => "A", :temp => 20 } at 11:00:00 am,
  { :room_id => "B", :temp => 21 } at 11:01:00 am,
  { :room_id => "A", :temp => 23 } at 11:02:00 am,
  { :room_id => "B", :temp => 22 } at 11:03:00 am]
\end{verbatim}

\verb=humidity=, on the other hand, contains:

\begin{verbatim}
 [{ :room_id => "B", :hum => 60 } at 11:00:30 am,
  { :room_id => "A", :hum => 62 } at 11:01:30 am,
  { :room_id => "A", :hum => 61 } at 11:02:30 am,
  { :room_id => "B", :hum => 59 } at 11:03:00 am]
\end{verbatim}

Now imagine you need to correlate temperatures and humidities. For
instance, you may need to answer the following question: ``what was
the maximum temperature while the humidity was above 60\%?''. To do
this, it might come in handy to ``merge'' both streams to obtain a new
stream with three fields --- \verb=:room_id=, \verb=:temp= and
\verb=:hum=. The events in this new stream are taken from both
streams and are sorted by timestamp. You can do this with:

\begin{verbatim}
  temps_and_hums = temperature
                     .outerJoin(humidity,
                                [:room_id, :room_id], :timestamp)
\end{verbatim}

This results in:

\begin{evaluation}
ezql> temps_and_hums
[\{ :room_id => "A", :temp =>  20, :hum =>     \} at 11:00:00,
 \{ :room_id => "A", :temp =>    , :hum =>  62 \} at 11:00:30,
 \{ :room_id => "B", :temp =>  21, :hum =>     \} at 11:01:00,
 \{ :room_id => "A", :temp =>    , :hum =>  62 \} at 11:01:30,
 \{ :room_id => "A", :temp =>  23, :hum =>     \} at 11:02:00,
 \{ :room_id => "A", :temp =>    , :hum =>  61 \} at 11:02:30,
 \{ :room_id => "B", :temp =>  22, :hum =>  59 \} at 11:03:00]
\end{evaluation}

Note that some fields don't have a value, they're empty or
blank. Empty fields mean there was no information for the field at
that moment. Empty fields are distinct from null values: null may be a
valid value in some situations. For example, suppose the humidity
sensor malfunctions and starts to emit null values. How would you
distinguish real null values like these from ``dummy'' nulls inserted
by the outer join? This is exactly what empty fields are for.

Unfortunately, the fact that some values may be blank influences the
results of correlations between these fields, a fact that restricts
the usefulness of this operator. For instance, if we try to answer our
original question, ``what was the maximum temperature while the
humidity was above 60\%?'' with the events above, the answer would be
``null'' because, as far as the engine knows, everytime the humidity
was above 60\%, there was no information for the temperature. The
skeptic reader may question the usefulness of having empty fields. It
may seem they were created to solve a problem and failed. Still, they
get us halfway there. In chapter \ref{chap:objects} we will see how to
correcly answer the previous question and where does the solution make
good use of empty fields.

\subsection{Sliding windows}
\label{sec:sliding-windows}

Windows allow the developer to specify which parts of the stream he is
interested in. There are two types of sliding windows in EzQL:
temporal and fixed. Temporal windows select only the tuples within a
given timestamp. For example, to obtain the Apple tuples received in
the last 5 minutes, one could do:

\begin{verbatim}
  recent_apple = apple_stocks[5 min .. now]
\end{verbatim}

If this statement is evaluated at 11:05:00 am, \verb=recent_apple=
will contain:

\begin{evaluation}
  ezql> recent_apple
  [\{ :symbol => "AAPL", :price => 52 \} at 11:00:02 am,
   \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

Fixed windows, on the other hand, select tuples based on their
position. The following code selects the last 3 elements received in
the original \verb=stocks= stream:

\begin{verbatim}
  last_3 = stocks[0 .. 2]
\end{verbatim}

Note that streams are 0-based. The previous example will result in:

\begin{evaluation}
  ezql> last_3
  [\{ :symbol => "MSFT", :price => 70 \} at 11:00:03 am,
   \{ :symbol => "MSFT", :price => 75 \} at 11:00:06 am,
   \{ :symbol => "AAPL", :price => 50 \} at 11:00:07 am]
\end{evaluation}

Just like aggregators, windows automatically update themselves as time
passes or new events arrive.

In the future, it should also be possible to support more complex
definitions of windows. For instance, ``all events from the past
week'', ``all events from Monday up to yesterday'', ``the fifty latest
events from the last 10 minutes'', etc.

Windows are just special kinds of streams where events ``expire'' and
are removed. Thus, most stream operations are also available on
windows. One could, for instance, count the number of events received
per company over the last 5 seconds with:

\begin{verbatim}
  count_per_company =
    stocks[5 sec].groupby(:symbol,
                          \group -> group.count())
\end{verbatim}

If this statement is evaluated at 11:00:08 am, it will result in the
following dictionary:

\begin{tabular}{ |l|c| }
  \hline
  :symbol & count() \\
  \hline
  ``AAPL'' & 1 \\
  ``MSFT'' & 2 \\
  \hline
\end{tabular}

Note also that the statement is equivalent to:

\begin{verbatim}
  count_per_company =
    stocks.groupby(:symbol,
                   \group -> group[5 sec].count())
\end{verbatim}

That is, applying the window before or after the \emph{group by}
returns the same result.

\subsection{The flux operator}
\label{sec:flux}

The flux (\verb=|>=) operator is basically just a replacement for the
. (dot) operator that suggests the idea of a pipeline where tuples
flow from operator to operator. For example, to filter all Apple
tuples and convert the \verb=:price= from dollars to euros and then
obtain the average, one could write:

\begin{verbatim}
  stocks
    .where  (\t -> t.symbol == "AAPL")
    .select (\t -> { t with :price => dol2eur(t.price) })
    .avg    (:price)
\end{verbatim}

With the flux operator, the same query could be written as:

\begin{verbatim}
  stocks
    |> where  (\t -> t.symbol == "AAPL")
    |> select (\t -> { t with :price => dol2eur(t.price) })
    |> avg    (:price)
\end{verbatim}

\subsection{Light syntax}
\label{sec:light-syntax}

Up until now we have been using the operators explicitly, as they
really are: functions that receive arguments --- which may themselves
be other functions ---, and that act on streams to generate other
streams, dictionaries, windows or continuous values.

EzQL supports an alternative syntax resembling SQL, henceforth called
\emph{light syntax}. A query that filters all Apple tuples and
converts the ``price'' from dollars to euros looks like this written
in the light syntax:

\begin{verbatim}
  from   t in stocks
  where  t.symbol == "AAPL"
  select { t with :price => dol2eur(t.price) }
\end{verbatim}

This ``light'' syntax is really just syntactic sugar: the compiler
translates code from the light to raw syntax before processing.

From now on we will use light syntax in all our examples.

\chapter{Modeling entities with objects}
\label{chap:objects}

\section{Motivation}

In section \ref{sec:timestamp-join}, we were given two streams ---
\verb=temperatures= and \verb=humidity= ---, containing temperature
and humidity readings from a set of rooms in a building. We then
failed to provide an answer to an apparently simple question: ``what
was the maximum temperature while the humidity was above
60\%?''. Using a timestamped outer join seemed like the right thing to
do, but this approach failed as some fields in the resulting stream
were blank.

However, in this scenario, we are dealing with physical quantities
measured frequently. It wouldn't be completely off the line to assume
that temperatures and humidities retain their previous values until
new updates arrive. This approach would work but, in general, we can't
make these kind of assumptions. Sometimes the inexistence of an event
really means that nothing has happened (orders not placed, earthquakes
that didn't happen, \ldots) so it's wrong to assume the previous value
remains the same in the general case.

Joining streams has another weakness. In this example we only had to
deal with temperatures and humidity readings. But imagine we need to
take other kinds of sensors into account: atmospheric pressure, noise
level, \ldots. To use all these different readings in a query, we
would have to join them all. Furthemore, to perform calculations over
each room independently, we need an additional \emph{group by}. All
these \emph{join}s and \emph{group by}s difficult the understanding of
queries and hide their real intention, i.e., that there are rooms with
temperatures and humidity levels and we want to compute results for
each room individually based on those values.

There are two problems here. First, we have an entity --- the room
---, whose properties --- temperature and humidity ---, are scattered
around multiple streams and need to be merged together for further
processing. Having multiple rooms aggravates the issue, because the
properties of multiple entities become all mixed up. Second, events
aren't supposed to carry stateful information. Events represent
isolated occurrences. A temperature reading of 20 degrees means that
the temperature at that time was 20 degrees, but it doesn't say
anything about the evolution of temperature in the time that goes from
this event to the next. We can't just ``fill the gaps'' and we can't
assume ``one size fits all'' because, as said above, sometimes it
makes sense to assume some value remains constant between updates,
sometimes it doesn't.

What we really need to solve both problems is some way to gather all
the related information together and to explicitly manage stateful
attributes and how they evolve over time. What we really need is to be
able to model the scenario using objects.

\section{Simple objects}
\label{sec:simple-objects}

From an OOP perspective, it is clear that our scenario could be
modeled with many Room objects (one per each physical room) with
temperature and humidity attributes.

In EzQL, this could be accomplished with:

\begin{verbatim}

temps_and_hums = temperature
                   .outerJoin(humidity,
                              [:room_id, :room_id], :timestamp)

@createFrom(temps_and_hums, onNew = :room_id)
class Room
end
\end{verbatim}

This code declares a new class, \verb=Room=, but leaves the
declaration empty. However, is also tags the class with one
annotation, \verb=@createFrom=. This signals the ESP engine to
automatically create new \verb=Room= instances when previously unseen
\verb=room_ids= show up in the stream \verb=temps_and_hums=.

In addition, this annotation automatically adds the stream fields to
the class. So, every Room instance ends up with three attributes:
\verb=room_id=, \verb=temp= and \verb=hum=. These attributes are
automatically filled and updated by the system as new events
arrive. Naturally, the values of these attributes are independent from
object to object. That is, an event with \verb=room_id= equal to 2
will only affect the Room object with \verb=room_id= equal to 2 and
not the others.

Having set up this apparatus, we can now run the simulation and see
the objects being created for us automatically. All the \verb=Room=
instances are accessible using \verb=Room.all=. In essence, this is a
dictionary that maps object id's (taken from the \verb=:room_id= field
in this case) to the objects themselves. If the stream \verb=temps_and_hums= equals:

\begin{evaluation}
ezql> temps_and_hums
[\{ :room_id => "A", :temp =>  20, :hum =>     \} at 11:00:00,
 \{ :room_id => "A", :temp =>    , :hum =>  62 \} at 11:00:30,
 \{ :room_id => "B", :temp =>  21, :hum =>     \} at 11:01:00,
 \{ :room_id => "A", :temp =>    , :hum =>  62 \} at 11:01:30,
 \{ :room_id => "A", :temp =>  23, :hum =>     \} at 11:02:00,
 \{ :room_id => "A", :temp =>    , :hum =>  61 \} at 11:02:30,
 \{ :room_id => "B", :temp =>  22, :hum =>  59 \} at 11:03:00]
\end{evaluation}

then \verb=Room.all= will return:

\begin{tabular}{ |l|l| }
  \hline
  id & object \\
  \hline
  ``A'' & Room with room\_id = ``A'', temp = 23, hum = 61 \\
  ``B'' & Room with room\_id = ``B'', temp = 22, hum = 59 \\
  \hline
\end{tabular}

Note that the temperature inside the room ``A'' is 23 degrees even
though the last event for this room contained an empty \verb=:temp=
field. If you recall, in section \ref{sec:timestamp-join} empty fields
were introduced to represent the absence of information. In this
example, an empty field means the temperature sensor didn't emit
anything at 11:03:00 am. So, it's safe to ignore these fields (after
all they were created by the \emph{outer join}, anyway) and assume the
temperature remains the same 23 degrees as in the previous
report. It's like in a regular OOP programming language: if an
attribute isn't modified, it retains the previous value. This is not a
case-specific rule, it's global: objects always ignore empty fields
(but they don't ignore null values).

Naturally, we can query these objects as if they were normal streams:

\begin{verbatim}
  from   room in Room.all
  where  room.hum > 60
  select room.room_id
\end{verbatim}

This would result in a subset of \verb=Room.all= containing only one
room:

\begin{tabular}{ |l|l| }
  \hline
  id & object \\
  \hline
  ``A'' & Room with room\_id = ``A'', temp = 23, hum = 61 \\
  \hline
\end{tabular}

Previously, we said that all \verb=Room= instances automatically get
three fields from the stream \verb=temps_and_hums=. This is the
default behavior, as it will most likely be the intention of the
developer. But this behavior may be overriden if desired. Furthermore,
one may create new fields whose values are taken from an arbitrary
stream. For example, to create all the fields manually one could do:

\begin{verbatim}
@createFrom(temps_and_hums, onNew = :room_id, autoCreate = false)
class Room
  @field(from t in temps_and_hums select t.room_id)
  roomId

  @field(from t in temps_and_hums select t.temp)
  currentTemperature

  @field(from t in temps_and_hums select t.hum)
  currentHumidity
end
\end{verbatim}

Note how you can specify from which stream to get the attribute values
using the \verb=@field= annotation.

\section{Associations}
\label{sec:associations}

Suppose now that you are trying to model a scenario where there is a
factory with many rooms. Products follow a predetermined path along
the pipeline and may switch rooms frequently. Every product has a
RFID tag and every room's entry has RFID readers which emit events to
the \verb=entries= stream declared with:

\begin{verbatim}
 entries = new Stream of { :room_id, :product_id }
\end{verbatim}

The question you need to answer is simply ``What is the temperature at
the room where product X is?''.

This is not so difficult using regular stream operations:

\begin{verbatim}
product_to_room = entries
                    .groupby(:product_id,
                             \group -> group.last)

Room.all[product_to_room["X"]].temp

TODO: Explain last. Returns the latest event in a stream
\end{verbatim}

But if you need to know the number of products in each room, you'd
have to create the opposite relationship, \verb=room_to_product=:

\begin{verbatim}
room_to_product = product_to_room
                    .flatten
                    .groupby(:room_id,
                    \group -> group.count())

TODO: explain flatten. Basically, it turns a dictionary produced
by group by into a regular window:

if product_to_room =

product_id -> room_id
"X"        -> "G53"
"Y"        -> "A"
"Z"        -> "A"

product_to_room.flatten =

[{ :product_id => "X", :room_id => "G53" },
 { :product_id => "Y", :room_id => "A" },
 { :product_id => "Z", :room_id => "A" }]

But is it a regular window? I mean, can you do .flatten[5 min] ?

\end{verbatim}

Clearly, this works, but it would be much easier if our objects could
be enriched with associations to each other. In this case, we'd like
to add a list of \verb=Product=s to every \verb=Room= and a reference
to a \verb=Room= to every product. This can be done in EzQL as
follows:

\begin{verbatim}
@createFrom(temps_and_hums, onNew = :room_id)
@hasMany(:products)
class Room
end

@createFrom(entries, onNew = :product_id)
@belongsTo(:room)
class Product
end
\end{verbatim}

The \verb=@hasMany= annotation means that each room may contain an
unbounded number of products. This annotation also adds a list of
\verb=Product= instances the every \verb=Room=. So, if you want to get
the list of products in room ``A'' you can do so with:

\begin{verbatim}
Room.all["A"].products
\end{verbatim}

The \verb=@belongsTo= annotation adds the reverse association, i.e.,
it means that every product is inside one room. Moreover, this
annotation also adds an attribute \verb=room= to every
\verb=Product=. Thus, to know in which room is product ``X'' one could
write:

\begin{verbatim}
Products.all["X"].room
# or ...
Products.all["X"].room.room_id  # ... to get just the ID
\end{verbatim}

And you can find the number of products per room using:

\begin{verbatim}
from   room in Room.all
select room.room_id, room.products.length
\end{verbatim}

Certainly an improvement. Not only is the query more concise, it is
also much simpler to understand.

All these attributes created automatically are managed by the engine
without any additional effort required from the programmer. All of
this may seem like a bit of hidden magic, but it's really just a
simple convention:

\begin{enumerate}
\item The \verb=:products= parameter to the \verb=@hasMany= annotation
  instructs the engine to create a list of \verb=Product= instances
  (it's simply the class whose name is the singular of
  ``products''). Furthermore, this list will be named ``products'';
\item On the other hand, the \verb=:room= parameter to the
  \verb=belongsTo= annotation tells the engine to create a field named
  ``room'' in every product. This field will hold an instance of the
  \verb=Room= class (once again, it finds the class name through the
  parameter);
\item How does the engine fill the \verb=room= attribute of every
  \verb=Product=? Easy, it checks the \verb=room_id= field. Remember,
  this field was created automatically by the \verb=@createFrom=
  annotation. Given the identifier of the room, the engine can easily
  lookup the corresponding \verb=Room= instance using the
  \verb=Room.all= dictionary;
\item Finally, after knowing where each product is, it is
  straightforward to invert the association to find out which products
  are in which rooms.
\end{enumerate}

Naturally, this convention may be overriden.

% TODO: give examples
% Which other relation types are supported? Give examples.

\chapter{Continuous values}
Just as you may query windows containing old events, you may reference
old attribute values in your queries. For example, you can calculate
the maximum temperature of room ``A'' over the last 5 minutes using
the \verb=temp= attribute from the \verb=Room= class as follows:

\begin{verbatim}
Room.all["A"].temp[5 min].max()
\end{verbatim}

You could have solved this using standard stream operators presented
in chapter \ref{chap:streams-events}:

\begin{verbatim}
from   ev in temperature
where  ev.room_id = "A"
select max(ev.temp)
\end{verbatim}

Surprisingly though, the results would have been different. To
understand why, imagine the \verb=temperature= stream contains the
following events:

\begin{verbatim}
 [{ :room_id => "A", :temp => 25.0 } at 10:54:00 am,
  { :room_id => "A", :temp => 24.9 } at 10:57:00 am]
\end{verbatim}

Also, assume that between events the temperature remains constant. In
the real-world there are always small oscillations, but imagine the
sensor is prepared to ignore these and emit new results only when they
differ from the previous one by a given amount (0.1 degrees in this
case).

Now, suppose the question is posed at 11h am and no further updates
were received. What was the maximum temperature over the last 5
minutes? Intuitively, you would answer 25 degrees, because that was
the temperature 5 minutes before. However, the event that contains
that temperature is, as of 11h am, outside the window. So, using just
regular sliding windows on streams will return 24.9 --- the wrong
result.

Objects, on the other hand, behave differently. As mentioned in
section \ref{sec:simple-objects}, an object's attribute retains its
value until a new event changes it. One could plot the evolution of an
attribute over time as a step function that changes when new events
arrive. Figure [something] illustrates this for the
\verb=Room.all["A"].temp= attribute. As the plot shows, the
temperature at 10:54:00 am became 25 degrees and remained there until
10:57:00 am, well inside the 5 minutes window. Thus, querying the
attribute for its maximum value over the last 5 minutes would return
25.

[TODO Picture]

This is surprising because objects were introduced with the intention
of simplifying queries. But objects also introduce state and this
seemingly innocuous change has profound implications. In essence,
events are discrete while attributes are continuous --- i.e., their
value is always defined. With objects, writing many state related
queries --- like the one in this example --- becomes easier than using
only regular stream operations.

Attributes belong to a broader class of expressions called
\emph{continuous values}, which we may define as expressions whose
past history is well known. This past history includes not only the
past values the expressions evaluated to, but also the time interval
in which they did. Continuous values provide an elegant way to solve
many time related queries, as we will see next.

\section{Aggregators over continuous values}

The fact that continuous values retain their previous values allows
one to compute aggregations over those values as we saw in the
previous section. However, continuous values hold two dimensions of
information: their previous values and the time intervals those values
hold. The traditional aggregations we are used to --- \verb=max=,
\verb=avg=, \verb=sum=, \ldots ---, only care for the previous
values. But we can exploit the time intervals information to create
and experiment with new kinds of aggregators. For example, imagine you
want to know for how long was the temperature equal to 25 degrees. We
could create a \verb=sumIntervalsWhereValueIs= aggregator that checks
the past history of the temperature and sums the intervals where it
was equal to some parameter. Then, answering the question would be as
simple as:

\begin{verbatim}
Room.all["A"].temp.sumIntervalsWhereValueIs(25)
\end{verbatim}

Or you could find the biggest interval where the temperature was 25
degrees using another aggregator,
\verb=lengthOfMaxIntervalWhereValueIs=:

\begin{verbatim}
Room.all["A"].temp.lengthOfMaxIntervalWhereValueIs(25)
\end{verbatim}

These aggregators have rather lenghty names and they have a few
limitations. For instance, they only consider intervals with the same
value. What if you need to know for how many minutes was the
temperature above or below a certain threshold?

\section{Continuous queries produce continuous values}

Suppose you need to know if the current temperature is above or equal
to 25 degrees:

\begin{verbatim}
Room.all["A"].temp >= 25
\end{verbatim}

This expression it will return either \verb=true= or \verb=false=. But
what happens if the expression is used as part of a continuous query?
If you remember from section [something], a continuous query is always
running and producing results. Hence, the query should produce
\verb=true= when the temperature reaches 25 degrees and \verb=false=
when it decreases below 25. That is, in the continuous context, the
entire expression has a value everytime the continuous attribute,
\verb=Room.all["A"].temp=, is defined. The following table shows this
correspondence:

\begin{tabular}{ |l|r|r| }
  \hline
  Time interval & \verb=Room.all["A"].temp= & \verb!Room.all["A"].temp >= 25! \\
  \hline
  $[$10:50; 10:53) & 25.0 & \verb=true=  \\
  $[$10:53; 10:55) & 24.9 & \verb=false= \\
  $[$10:55; 10:56) & 24.8 & \verb=false= \\
  $[$10:57; 10:59) & 24.9 & \verb=false= \\
  $[$10:59; 11:00) & 25.0 & \verb=true=  \\
  \hline
\end{tabular}

The boolean expression has a current value as well as a collection of
previous values. By definiton then, the boolean expression is, itself,
a continuous value. So we can just use new aggregators like the ones
defined in the previous section to perform arbitrary computations on
these values. For example, to sum the time intervals where the
temperature was equal or above 25 degrees, you could use the
\verb=sumIntervals= aggregator, which sums up the intervals where the
value is \verb=true=:

\begin{verbatim}
(Room.all["A"].temp >= 25).sumIntervals()
\end{verbatim}

Note that to find out for how long was the temperature equal to 25
degrees, you could now write:

\begin{verbatim}
(Room.all["A"].temp == 25).sumIntervals()
\end{verbatim}

% TODO mention that this is done incrementaly, unlike temporal databases which are ad-hoc

This is certainly more understandable than the
\verb=sumIntervalsWhereValueIs= aggregator defined in the previous
section.

\section{Composite attributes}

Let's go back to the ``products inside rooms'' example of section
\ref{sec:associations}. Suppose we have the following sequence of
events arriving into the \verb=entries= stream:

\begin{verbatim}
[{ :product_id => "X", :room_id => "A" } at 10:00 am,
 { :product_id => "X", :room_id => "B" } at 10:10 am
 { :product_id => "X", :room_id => "C" } at 10:17 am]
\end{verbatim}

That is, the product ``X'' enters room ``A'' at 10 am, 10 minutes
later it enters room ``B'' and finally, at 10:17 am it enters room
``C''. After all these events are received, the attribute
\verb=Product.all["X"].room_id= will be a continuous value with the
following history:

\begin{tabular}{ |l|r| }
  \hline
  Time interval & \verb=Product.all["X"].room_id= \\
  \hline
  $[$10:00; 10:10) & ``A'' \\
  $[$10:10; 10:17) & ``B'' \\
  $[$10:17;   now) & ``C'' \\
  \hline
\end{tabular}

Suppose we enrich our \verb=Room= instances with a temperature
attribute which gets its value from the temperature of the room the
product is in:

\begin{verbatim}
@createFrom(entries, onNew = :product_id)
@belongsTo(:room)
class Product
    temperature = this.room.temp
end
\end{verbatim}

Finally, assume the following temperature readings are received in the
\verb=temperatures= stream:

\begin{verbatim}
 [{ :room_id => "A", :temp => 10 } at 09:50:00 am,
  { :room_id => "B", :temp => 15 } at 10:09:00 am,
  { :room_id => "C", :temp => 20 } at 10:16:00 am]
\end{verbatim}

If we correlate both streams, we see that the temperature in room
``A'' was 10 degrees while the product ``X'' was there, the
temperature in room ``B'' was 15 degrees while the product was there
and finally, the temperature in room ``C'' was 20 degrees while the
product was there. Since the temperature of a product was defined as
the temperature of the room the product is in, the \verb=temperature=
attribute for product ``X'' will contain the following history:

\begin{tabular}{ |l|r|r| }
  \hline
 Time interval & \verb=Product.all["X"].room_id= & \verb=Product.all["X"].temperature= \\
  \hline
  $[$10:00; 10:10) & ``A'' & 10 \\
  $[$10:10; 10:17) & ``B'' & 15 \\
  $[$10:17;   now) & ``C'' & 20 \\
  \hline
\end{tabular}


\chapter{User defined functions and aggregators}
\begin{itemize}
\item Explain the need for new aggregates (forMoreThan())
\item Explain the differences between continuous and batch mode
\item Explain the aggregates
\item schedule?!
\end{itemize}
\chapter{Reference}

\section{Stream}

\section{Window}

\section{Dictionary}

\section{Continuous values}

\chapter{Bibliography}
\end{document}
