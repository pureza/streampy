\documentclass{report}


\usepackage{alltt}

\begin{document}
\title{StreamPy: A new language for Event Stream Processing}
\author{Lu\'{\i}s Pureza}

\maketitle

\tableofcontents

\addtolength{\parskip}{\baselineskip}
\chapter{Introduction}

Technology developments and its widespread adoption over the last
decades have significantly increased the demand for information
processing systems. Not only do we now need to process larger amounts
of information coming from everywhere, we must do it faster as
well. For some companies, obtaining results a few milliseconds earlier
may be a significant advantage over the competition. For others,
however, reacting immediately is of critical importance, as it
happens, for example, in the case of security breaches or nuclear
power plant malfunctions.

One particular class of applications, now referred to as Event Stream
Processing (ESP), has been the subject of much attention over the last
few years due to the technical challenges it presents. ESP
applications are characterized by dealing with a possible infinite
amount of data constantly flowing in to be processed as fast as
possible to continuously produce new and updated results that may
themselves be used to justify new decisions. It turns out that many
applications fit naturally in this model: financial analysis,
health-care monitoring, network intrusion detection, personnel and
product tracking through RFID devices, business monitoring and many
more.

Due to the inability of current technology to satisfy the increasing
demands from all these markets, computer scientists developed the
first Data Stream Management Systems (DSMS). Coming mostly from the
database community, these researchers intended to build a generic
engine that abstracted away all the low level details of managing
streams of data in high demanding scenarios, while retaining much of
the querying capabilities of regular Database Management Systems
(DBMS), so that they could be easily adapted to a multitude of
domains. It's no surprise then, that the first DSMSs inherited a SQL
dialect with some new extensions. Aurora [ref], with its boxes and
arrows graphical queries was the exception, but the operators it
provided still took inspiration from SQL. Later, when the first DSMS
hit the market, some emphasis was put on end-user interaction and some
applications began to include rule-based systems that allow the
developer to specify how he wants to react to events using Event
Condition Action (ECA) rules, a concept developed in the context of
active databases [ref]. At the same time, DSMS began to incorporate
features from Complex Event Processing (CEP) systems, that allowed the
user to detect complex patterns and correlations among the input
streams of data. This required adding new constructs into the query
language. Recently, some companies unhappy with declarative, SQL-like
languages, began to support procedural languages, more familiar to C
and Java programmers. Nowadays, each product includes its own flavor
of a SQL-like language with its own unique and esoteric extensions, or
a rule-based language, or a procedural language, or any combination of
these three. Besides the obvious issue created by the lack of a
standard, all this variety demonstrates that DSMS applications have
their own needs and a satisfactory end-user query language for them is
yet to be found.

To make matters worse, the semantics on many of these products
disagree in fundamental ways. In [ref], researchers analyzed how two
of the most prominent products from Oracle and StreamBase reacted in
the presence of simultaneous events. Surprisingly, they found that in
some scenarios, they are both wrong! Furthermore, they concluded that
this disparity may be blamed on the semantics employed by each
product. It also happens that many products consistently implement
inappropriate semantics. For example, as we will see in section
\ref{state-of-the-art}, DSMS typically provide ways of analyzing data
newer than some threshold -- 3 minutes, 5 hours, 10 days,
etc. However, naively applying these constructs to answer a simple
question such as ``Was the temperature above 20 $^{\circ}$C during any
moment of the last 5 minutes?'' will give wrong results. Will see why
in chapter \ref{motivation}.

Despite these limitations, more and more organizations are adopting
DSMS and relying on them to process data coming from everywhere --
including core business processes. As a consequence, ESP applications
tend to grow and become more complex. However, it is our opinion that
languages available in currently existing products are not prepared to
support such large applications. It's just not the problem with the
semantics. Simply put, they were not designed for that. In chapter
\ref{state-of-the-art} we will highlight many pitfalls of existing
solutions and even show a couple of perfectly reasonable queries in
the context of ESP applications, expressible in one sentence of
English, which are very difficult to implement. If they fail in those
situations, how could they handle complex applications?

The purpose of this internship is to design a new query language for
ESP systems. This new language combines powerful constructs, with some
innovative features that, we hope, should be a step in the right
direction to create a language that developers find rich, expressible
and coherent. The rest of this thesis is organized as follows:

\begin{itemize}
\item In chapter \ref{state-of-the-art}, we will expand on the
  previous paragraphs to detail the state-of-the-art and to show that
  art is indeed very subjective and multifaceted;
\item Chapter \ref{motivation} discusses some limitations crippling
  currently available engines and provides some hints as to how they
  could be fixed;
\item Our proposal will then be introduced in chapter \ref{streampy},
  first through the analysis of a few examples followed by a thorough
  discussion of all its features;
\item Further work and planning for the next semester will be the
  topic of chapter \ref{future-work};
\item Finally, chapter \ref{conclusion} concludes this document.
\end{itemize}

By the end of this document, it should be clear that not only current
technology will fail short of fulfilling the increasing needs of
costumers, but also that our proposal contains some interesting
characteristics that deserve further attention.

\chapter{State-of-the-art}
\label{state-of-the-art}

This section will analyze SQL dialects present in some industry
products for event processing - Coral8, StreamBase and Esper. Then,
StreamBase EventFlow, a method for creating queries visually through a
boxes and arrows interface will be discussed. Despite not involving
programming languages, it's important to study these alternatives
because, after all, they're trying to address the same problem. Next
comes Oracle Rules Manager with its rule-based paradigm for complex
event processing. We will conclude this chapter analyzing
MonitorScript, a procedural language developed by Progress Apama to
overcome the limitations imposed by SQL.

\section{SQL-based languages}

The most popular way of writing queries is through a declarative,
SQL-based language. Products adopting this solution include Coral8
[ref], StreamBase [ref] and Esper [ref]. As explained in the
introduction, this comes as a result of all the effort the database
community has put into creating the first ESP engines. Currently there
is no standard for a query language designed specifically for these
applications which means each vendor has their own. Fortunately,
they're all very similar. The simplest things can be done exactly as
in regular SQL:

\begin{verbatim}
INSERT INTO PriceMicrosoft
SELECT *
FROM StockTrades
WHERE symbol = 'MSFT'
\end{verbatim}

This query looks for tuples where the field \verb=symbol= equals
``MSFT'' arriving at the stream \verb=StockTrades= and adds them to
the \verb=PriceMicrosoft= stream. It's just a simple filter.

One feature that assumes particular importance in the context of ESP
is the concept of window. Windows allow developers to consider only
part of a stream and make computations over it. In particular, it
becomes possible, for example, to calculate the most expensive order
among the last million, or the average of Microsoft's stock prices
over the last 3 hours, as the following example shows:

\begin{verbatim}
INSERT INTO AvgPriceMicrosoft
SELECT avg(price)
FROM StockTrades KEEP 3 HOURS
WHERE symbol = 'MSFT'
\end{verbatim}

It's possible to customize the retain policy and keep, for example,
the 10 largest updates based on their price, the 10 last updates per
company or simply to keep everything.

The window defined in the previous example is a \emph{sliding}
window. Sliding, in this context, refers to how and when the elements
in the window expire. In these kinds of windows, tuples are removed
from the window as they become too old or new tuples arrive. An
alternative is \emph{jumping} windows, where tuples are appended to
the window and then, when the oldest element was supposed to be
removed, all of them expire at the same time, the window becomes empty
and the cycle begins all over again. This way, it's possible to find
out the largest price attained by Microsoft stocks over the last hour,
the hour before and so on.

All the examples in this section were written in Coral8 Continuous
Computation Language (CCL). Nonetheless, all the other systems in this
section provide similar features albeit with different syntax. In
fact, there have been talks about creating a standard language lately,
an idea that was met with some skepticism by some vendors and users
that don't like SQL anyway and opted for other approaches.

SQL based solutions come with a few problems, however. First,
experience tells that SQL queries easily become overly complex and
hard to understand. Second, SQL's declarative nature doesn't always
map well with the way programmers think. Sometimes, things that are
simple to do in C or Java become ugly hacks when translated to SQL,
because SQL, despite being Turing-complete, just wasn't made to
compete with those languages. We will develop these arguments while
exploring a simple example (adapted from [ref]).

Robay, a fictional online auction company, is experimenting with a
DSMS to manage auctions. In their setup, they have two relations --
\verb=Item(id, name, categoryId)= and
\verb=Category(id, name, description)= and three streams --
\verb=OpenAuction(itemId, start_price)=, \verb=ClosedAuction(itemId)=
and \verb=Bid(itemId, bid_price)=. Information on sellers and buyers
has been omitted because it won't be necessary to illustrate our
point. One of the first queries Robay developers wrote returns, for
each closed auction, its item and category ids, along with the price
offered in the winner bid:

\begin{verbatim}
TODO: Implement this in Coral8 CCL

-- Unite Bid with OpenAuction
ItemPrices:
    Select itemID, bid_price as price 
    From   Bid 
    Union All
    Select itemID, start_price as price 
    From OpenAuction

-- Get the current price for each product
CurrentPrice: 
    Select    P.itemID, Max(P.price) as price
    From      ItemPrices P
    Group By  P.itemID

-- Get the product, category and final price for each closing auction
ClosingPriceStream:
    Select   Rstream(T.id as catID, P.price as price)
    From     ClosedAuction [Now] C, CurrentPrice P, 
             Item I, Category T    
    Where    C.itemID = P.itemID and C.itemID = I.id and I.categoryID = T.id 
\end{verbatim}

This solution uses regular SQL extended with windowing constructs and
should be pretty easy to understand. Despite being so straightforward,
there are a few issues with it that could be improved. First of all,
notice that besides the \verb=ClosingPriceStream= -- the desired
result, two other temporary streams had to be created. They are not
strictly necessary and their code could be directly inserted in the
\verb=FROM= clause of the \verb=ClosingPriceStream=, but that would
quickly result in unreadable code. To avoid creating big and complex
queries, developers partition them into smaller ones connected by
temporary streams. Thus, streams end up being used as variables to
store the intermediate results of computations. But streams aren't a
suitable replacement for all kinds of variables. In procedural
programming languages, programmers can choose among primitive
variables (int, float, string, \dots), arrays, dictionaries, lists and
sets among others, to manage information. Furthermore, Object-Oriented
Programming brought classes and objects to the table, important not
only to keep information in an organized way, but also because they
allow developers think in terms of the real world and model objects
and the relations between them. Certainly, streams and tables alone
could be used to simulate all these kinds of constructs, but they're
just not the right tool for \emph{every} job.

Second, notice the query for the \verb=CurrentPrice= stream. To find
the current price, it first finds the starting price, then all the
prices from later bids and finally it chooses the largest, which is
also the latest. Notice how the query builds, implicitly, the current
state of each item. This is necessary because the DSMS doesn't know
what an item really is and doesn't understand the relation between two
updates for the same item. That's why it is necessary, in order to
find the current price, to instruct the system to go back and get the
price from the latest event. From an OOP point-of-view, this query
would be greatly simplified if there was some way to define
\verb=Item= objects with properties like \verb=current_price= to hold
the state of each item and use these objects in queries, just like
regular tuples. Moreover, these objects and their properties could be
automatically managed by the system, thus relieving the programmer
from lower-level tasks.

Third, notice the \verb=WHERE= predicate in the last query. This
predicate embodies three inner joins and should also be pretty clear
for any SQL programmer. The problem is that this join hides an
important fact about the system being modeled: objects have
relations. Auctions have one item and items have zero or more
bids. Relations are not new to the database community: DBMSs have
embraced and supported them for years. But database relations are
still considered too low-level by most programmers, who are used to
think in terms of objects instead of meaningless identifiers. This is
one reason why Object-Relational Mapping (O/RM) has flourished in the
last few years. If the developer could, somehow, model these
relations, the system would be able to enrich the objects mentioned in
the previous paragraph with pointers to the related objects and manage
these pointers automatically. With these changes, the last query would
become much more explicit and read something along the lines of ``For
every closed auction, get the current price, category and id of its
item''.

It may seem that these issues aren't as bad as we're making
them. After all, the queries above are still easy to understand. Note
however, that the example is also very short. As explained in the
introduction, more and more code like this is being written like this
every day by organizations that rely on DSMS to manage their business
processes. As the code base grows, this mismatch between what
developers want and what the systems have to offer becomes more
evident to the point where, eventually, code becomes unmaintainable
and needs to be abandoned.

There are a few other issues besides those outlined above. We've seen
before that ESP systems provide many windowing facilities on top of
regular SQL -- sliding windows, jumping windows, windows per groups,
etc. Unfortunately, sometimes this is not enough, as is illustrated in
the following scenario. The Financial Association for Investments and
Little Experiments with DSMS is monitoring Robay stock actions. For
some contrived reason, they want to know the answer to the following
question: ``During the last period in which Robay actions stayed above
\$70, what was the average of their value?''. This would be easily
answered if we could create a window between the time stock actions
went above \$70 and the time they went below (or the current time, if
they're still above that mark). Notice that the endpoints for this
window would then be dictated by regular events, instead of being
fixed in time or size. These types of windows are called
\emph{semantic} windows [ref]. Unfortunately, currently available ESP
systems fail to provide support for semantic windows, making this kind
of queries difficult to solve.

Halted by this obstacle and unable to make progress, the association
decided to work on a easier question first: ``Were Robay actions above
\$70 during any moment of the last 5 minutes?''. This seems to be a
great use-case for temporal sliding windows. Unfortunately, the
semantics of these windows are not appropriate here, making the
solution overwhelmingly difficult. The problem lies, again, with the
lack of state. Suppose that, at 10:54, the system receives a
notification reporting that Robay's actions were worth \$71 and, 2
minutes later, it receives another event informing that the price
decreased to \$69. Suppose further that no other events arrived until
the question is posed, at 11:00 (of the same day). We, humans, know
that the answer is an obvious ``yes'' because Robay's actions were
valued at \$71 between 10:54 and 10:56. However, the system only cares
about events and the first notification lies outside the 5 minutes
window. Thus, this window will only include the second event and the
answer would be negative. If the system knew that, in between
notifications, stock actions keep their previous prices, it would
easily give the right answer. But it doesn't and so, solutions for
this simple problem involve managing the state explicitly, which is
certainly much more trouble than a simple window. Note that this is
analogous to the ``temperature above 20 $^{\circ}$C'' problem
mentioned in the Introduction.

To conclude this section, it should be said SQL has some benefits. For
simple things, its declarative nature makes it very expressible. After
all, queries represent, almost in English, the intention of the
developer. Also, SQL querying capabilities are very powerful and excel
in many data processing tasks. It is understandable then, that an
alternative language should take some inspiration from SQL and borrow
some features to provide equally powerful constructs.

\section{Building queries visually with StreamBase's EventFlow}

StreamBase employs an alternative, more user-friendly, way of building
queries. Instead of writing code, the user can design queries
visually, by arranging boxes in a canvas and connecting them with
arrows. Boxes represent operators that receive data coming from other
operators or directly from the input streams, process the data
according to its semantics and send the results to be handled by other
operators, or to the world as the output of the whole operation.

\emph{TODO Show picture}

Some of the most important operators are:

\begin{description}
\item [Aggregate] Computes aggregate operations over windows of
  tuples. Supports the option to partition tuples into sets and then
  apply the aggregation over each individual set, much like a SQL
  GROUP BY statement;
\item [Filter] Discards some tuples from the input stream based on a
  predicate. Performs the same function as a SQL WHERE statement;
\item [Gather] Receives tuples from two or more streams and
  concatenates those with the same key. The resulting tuple values may
  be direct copies from the original tuples or the result of applying
  some expression to these;
\item [Join] Similar to SQL joins, this operator pairs tuples coming
  from two streams that match a given condition and outputs a new
  tuple whose fields may be specified by the user. In general, joining
  two infinite streams may require keeping all tuples from both
  streams in memory. To avoid problems later on, the user must specify
  timing constraints regarding matching tuples saying, for example,
  that they must arrive withing 60 seconds of each other;
\item [Map] Similar to the first part of a SELECT statement, this
  operator transforms a tuple -- possibly discarding or adding new
  fields along the way, and sends it downstream;
\item [Pattern] Instructs the engine to look for complex events. This
  is a feature borrowed from Complex Event Processing and will be
  discussed below;
\item [Union] Acts as a multiplexer, connecting many input streams to
  a single output stream, to where all tuples are sent. Similar to the
  SQL UNION statement.
\end{description}

It's clear that most of these operators have SQL counterparts. There
are a few things that can be done in EventFlow but not in StreamSQL
[ref], but they're just small details, nothing that changes the
developer's way of thinking. It's then mostly a matter of taste -- not
power, to choose between StreamSQL and EventFlow.

\section{Oracle rules}

Oracle Rules Manager (ORM) embraces a completely different
paradigm. Instead of writing queries, the user creates rules that are
made of three parts. The first -- the event type, instructs the system
to trigger the rule only when an event of that type occurs. The
second, the condition, is a predicate that is compared against the
event. If they match, the third part, the action -- a PL/SQL
procedure, is executed.

Conceptually, a rule has the following format:

\begin{verbatim}
ON   <event>
IF   <condition>
THEN <action>
\end{verbatim}

The following example shows a simple rule:

\begin{verbatim}
ON   StockTick(symbol, price, volume)
IF   symbol = 'MSFT' and price > 100
THEN buyStocks(symbol)
\end{verbatim}

The event shown above represents a stock update received from some
external source. These are the simplest kind of events, also called
\emph{primitive} events. ORM also supports \emph{composite} events
that are defined as combinations of other events. For example, it is
possible to define a composite event that occurs when Microsoft shares
drop by 10\% and, less than 5 minutes later, IBM shares gain 2\%. To
be more specific, the following types of combinations are supported:
\begin{description}
\item [Sequencing] Specifies the order between events, i.e., event A
  must occur before event B;
\item [Negation] Checks for the non-occurrence of some event. Useful
  to raise exceptions when business rules are violated;
\item [Set semantics] Allows combining events with the AND operator to
  specify that all of them must occur for the composite event to occur
  as well;
\item [Any N] Checks for the occurrence of at least N children events;
\item [Collection] Combines a set of primitive events based on some
  common properties. Could be used, for example, to detect all
  costumers who withdrew more than \$1000 from their bank accounts
  during the past 24h.
\end{description}

ORM comes with a GUI utility to simplify the rules creation
process. This is necessary because creating a rule the hard way is a
daunting task that involves writing SQL queries containing XML blocks
containing SQL snippets inside. Still, there are situations when one
needs to avoid these utilities and work with real code, situations
where a simpler configuration process would be desirable.

The biggest disadvantage of rule-based languages comes from the
paradigm itself. While they may be effective for detecting complex
patterns of events, they are not so good when it comes to actually
processing the data. Built-in combinators provide some aggregation
functions, but these will seem basic for companies in need to
implement proprietary analysis algorithms. Also, PL/SQL is arguably
not a language suited for complex application development.

One other issue that plagues these systems is the difficulty to reason
about their runtime behavior. As the number and complexity of rules
increases, an event may trigger more than one rule and these rules may
themselves trigger other rules, resulting in a cascading process that
may never end. This behavior makes the applications more difficult to
understand, to the point where even adding or removing a simple rule
may have unpredictable effects. These kinds of non-linear interactions
are discouraged by Software Engineering best-practices and thus, the
decision to use rule-based systems for building large applications
must be carefully pondered. To be fair, Oracle Rules Manager provides
constructs to deal with simultaneous triggering rules, but the problem
doesn't go away: only now the developers will be to blame when things
go wrong.

\section{Apama's MonitorScript}

When ESP solutions left the academia to meet the real world, many
users argued that SQL was ill-suited for many information processing
tasks. MonitorScript (MS) is Apama's response to these complains.

Deriving from the procedural family of programming languages, MS
programs look a lot like Java programs. However, the most important
and unique feature it includes was borrowed from Erlang: the process
model based on having many little \emph{processes} that execute in
parallel and communicate with each other through messages. In MS,
these entities are called \emph{monitors}. A monitor is an event
processing agent that basically sets up an arbitrary number of
\emph{listeners} that await for the occurrence of events. When one
matching event arrives, monitors process it. Monitors can also send
events to each other, through the \emph{route} statement.

Last but not least, monitors can have state. This means that instead
of writing a SQL query to capture the state of the system, MS updates
the state incrementally as the computation moves forward. The downside
is that, for simple things, SQL queries will definitely be easier to
understand, because a SQL query \emph{is} its intention while, in a
procedural program, the big picture is not always that clear.

To get a feeling for the language, here is a small MonitorScript
snippet:

\begin{verbatim}
monitor ProcessMarket {
    // Keep the last price per company
    dictionary <string, int> lastPerCompany;

    action onload {
        Tick tick;
        on all Tick(): tick {
           processTick(tick);
        }

        on all PriceRequest(): ev {
            processPriceRequest(ev);
        }
    }

    action processTick(Tick tick) {
        lastPerCompany[tick.sym] := tick.price;
        route TickAck(tick.sym);
    }

    action processPriceRequest(PriceRequest ev) {
        if (lastPerCompany.hasKey(ev.sym)) then {
            route PriceReply(ev.sym, lastPerCompany[ev.sym]);
        }
    }
}
\end{verbatim}

This listing declares one monitor (\verb=ProcessMarket=), that waits
for \verb=Tick= and \verb=LastPriceRequest= events. When it receives a
new \verb=Tick= update, it keeps the price reported in a dictionary,
indexed by the company's symbol and replies with a \verb=TickAck=
message. When it receives a \verb=PriceRequest=, it fetches the last
price from the dictionary and sends a \verb=PriceReply= message.

Note the need to create request, reply and acknowledgement messages
when what one really wants to do is to perform a method invocation
between different monitors. Indeed, the runtime system could provide
some kind of remote procedure call mechanism and handle the low-level
details by itself.

MS and SQL are at opposite ends of the programming language
spectrum. While is simplifies a lot of tasks that don't naturally fit
into SQL, MS also looses all the querying capabilities that made SQL
and its derivatives popular for data processing.

\chapter{I want to call this ``A tale of two companies'' but I can't}
\label{motivation}

\chapter{The StreamPy query language}\label{streampy}

\chapter{Future work}\label{future-work}

\chapter{Conclusion}\label{conclusion}
\end{document}
